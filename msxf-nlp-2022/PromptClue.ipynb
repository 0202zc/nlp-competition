{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40cbef5-1289-42ed-97f5-28fe9377e6e5",
   "metadata": {},
   "source": [
    "# 使用自定义数据集训练PromptCLUE模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f521b0-e13d-4b4b-bb9b-064b78607439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end2...\n"
     ]
    }
   ],
   "source": [
    "# 引入相应的包 Importing libraries\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import time, json\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "# from models.modeling_t5 import T5ForConditionalGeneration\n",
    "\n",
    "# rich: for a better display on terminal\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "print(\"end2...\")\n",
    "\n",
    "# 计算rouge用\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7aab58-3d96-41f1-ae77-cfa9edf71ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end...\n"
     ]
    }
   ],
   "source": [
    "# 做一些相关的配置(打印显示；GPU设置)\n",
    "# define a rich console logger\n",
    "console = Console(record=True)\n",
    "\n",
    "# to display dataframe in ASCII format\n",
    "def display_df(df):\n",
    "    \"\"\"display dataframe in ASCII format\"\"\"\n",
    "\n",
    "    console = Console()\n",
    "    table = Table(\n",
    "        Column(\"source_text\", justify=\"center\"),\n",
    "        Column(\"target_text\", justify=\"center\"),\n",
    "        title=\"Sample Data\",\n",
    "        pad_edge=False,\n",
    "        box=box.ASCII,\n",
    "    )\n",
    "\n",
    "    for i, row in enumerate(df.values.tolist()):\n",
    "        table.add_row(row[0], row[1])\n",
    "\n",
    "    # console.print(table) # TODO TODO TODO \n",
    "\n",
    "# training logger to log training progress\n",
    "training_logger = Table(\n",
    "    Column(\"Epoch\", justify=\"center\"),\n",
    "    Column(\"Steps\", justify=\"center\"),\n",
    "    Column(\"Loss\", justify=\"center\"),\n",
    "    title=\"Training Status\",\n",
    "    pad_edge=False,\n",
    "    box=box.ASCII,\n",
    ")\n",
    "\n",
    "# Setting up the device for GPU usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d6c66-f468-43b9-8c03-d72f5594248b",
   "metadata": {},
   "source": [
    "# Dataset Class 自定义数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1040b801-29b9-4a4f-a31f-60fe11a85d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end...\n"
     ]
    }
   ],
   "source": [
    "class SmallSampleDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    创建一个自定义的数据集，用于训练，必须包括两个字段：输入(如source_text)、输出（如target_text）\n",
    "    Creating a custom dataset for reading the dataset and\n",
    "    loading it into the dataloader to pass it to the\n",
    "    neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Dataset class\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): Input dataframe\n",
    "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
    "            source_len (int): Max length of source text\n",
    "            target_len (int): Max length of target text\n",
    "            source_text (str): column name of source text\n",
    "            target_text (str): column name of target text\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text] if target_text is not None else None\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns the length of dataframe\"\"\"\n",
    "        if self.target_text is not None:\n",
    "            return len(self.target_text)\n",
    "        else:\n",
    "            return len(self.source_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if self.target_text is not None:\n",
    "            \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
    "\n",
    "            source_text = str(self.source_text[index])\n",
    "            target_text = str(self.target_text[index])\n",
    "\n",
    "            # cleaning data so as to ensure data is in string type\n",
    "            source_text = \" \".join(source_text.split())\n",
    "            target_text = \" \".join(target_text.split())\n",
    "\n",
    "            source = self.tokenizer.batch_encode_plus(\n",
    "                [source_text],\n",
    "                max_length=self.source_len,\n",
    "                pad_to_max_length=True,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            target = self.tokenizer.batch_encode_plus(\n",
    "                [target_text],\n",
    "                max_length=self.summ_len,\n",
    "                pad_to_max_length=True,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            source_ids = source[\"input_ids\"].squeeze()\n",
    "            source_mask = source[\"attention_mask\"].squeeze()\n",
    "            target_ids = target[\"input_ids\"].squeeze()\n",
    "            target_mask = target[\"attention_mask\"].squeeze()\n",
    "\n",
    "            return {\n",
    "                \"source_ids\": source_ids.to(dtype=torch.long),\n",
    "                \"source_mask\": source_mask.to(dtype=torch.long),\n",
    "                \"target_ids\": target_ids.to(dtype=torch.long),\n",
    "                \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
    "            }\n",
    "        else:\n",
    "            \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
    "\n",
    "            source_text = str(self.source_text[index])\n",
    "\n",
    "            # cleaning data so as to ensure data is in string type\n",
    "            source_text = \" \".join(source_text.split())\n",
    "\n",
    "            source = self.tokenizer.batch_encode_plus(\n",
    "                [source_text],\n",
    "                max_length=self.source_len,\n",
    "                pad_to_max_length=True,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            source_ids = source[\"input_ids\"].squeeze()\n",
    "            source_mask = source[\"attention_mask\"].squeeze()\n",
    "\n",
    "            return {\n",
    "                \"source_ids\": source_ids.to(dtype=torch.long),\n",
    "                \"source_mask\": source_mask.to(dtype=torch.long)\n",
    "            }\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb44c17-202a-46e2-86c2-0f301ead32b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 训练方法 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a69058b5-fe86-418f-a5b7-e12198436fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db736ef5-357f-4587-9ef2-d543b287c48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end...\n"
     ]
    }
   ],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer, scheduler, ema):\n",
    "\n",
    "    \"\"\"\n",
    "    用于训练的方法\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "\n",
    "    model.train()\n",
    "    time1 = time.time()\n",
    "    for _, data in enumerate(loader, 0):\n",
    "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
    "        y_ids = y[:, :-1].contiguous() # target, from start to end(except end of token, <EOS>). e.g. \"你好吗？\"\n",
    "        lm_labels = y[:, 1:].clone().detach() # target, for second to end.e.g.\"好吗？<EOS>\"\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100 # releted to pad_token and loss. for detail, check here: https://github.com/Shivanandroy/T5-Finetuning-PyTorch/issues/3\n",
    "        ids = data[\"source_ids\"].to(device, dtype=torch.long) # input. e.g. \"how are you?\"\n",
    "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            decoder_input_ids=y_ids,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "            \n",
    "        # 每100步打印日志\n",
    "        if _ % 100 == 0 and _ != 0 or _ == len(loader) - 1:\n",
    "            time2 = time.time()\n",
    "            print(\"Step:\", _,\"epoch: \" + str(epoch) + \"; loss:{:.4f}; each step's time spent:{:.2f}\".format(loss.detach().cpu().numpy(), float(time2-time1) / float(_ + 0.0001)))\n",
    "            # training_logger.add_row(str(epoch), str(_), str(loss))\n",
    "            # console.print(training_logger)\n",
    "            # console.log(f\"Step: {_}, epoch: {epoch}; loss: {loss.detach().cpu().numpy()}; each step's time spent:{float(time2-time1) / float(_ + 0.0001)}\\n\")\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ema.update()\n",
    "        scheduler.step()\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a96d01b-d310-45e6-a505-40ff1055a83a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 用于验证的方法 Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "425d1bb4-f139-4bdc-99da-700d04d70ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end...\n"
     ]
    }
   ],
   "source": [
    "metric_keys = ['main', 'rouge-1', 'rouge-2', 'rouge-l']\n",
    "\n",
    "def compute_rouge(source, target, unit='word'):\n",
    "    \"\"\"计算rouge-1、rouge-2、rouge-l\n",
    "    \"\"\"\n",
    "    # if unit == 'word':\n",
    "    #     source = jieba.cut(source, HMM=False)\n",
    "    #     target = jieba.cut(target, HMM=False)\n",
    "    source, target = ' '.join(source), ' '.join(target)\n",
    "    try:\n",
    "        scores = rouge.get_scores(hyps=source, refs=target)\n",
    "        return {\n",
    "            'rouge-1': scores[0]['rouge-1']['f'],\n",
    "            'rouge-2': scores[0]['rouge-2']['f'],\n",
    "            'rouge-l': scores[0]['rouge-l']['f'],\n",
    "        }\n",
    "    except ValueError:\n",
    "        return {\n",
    "            'rouge-1': 0.0,\n",
    "            'rouge-2': 0.0,\n",
    "            'rouge-l': 0.0,\n",
    "        }\n",
    "\n",
    "    \n",
    "def compute_metrics(source, target, unit='word'):\n",
    "    \"\"\"计算所有metrics\n",
    "    \"\"\"\n",
    "    metrics = compute_rouge(source, target, unit)\n",
    "    metrics['main'] = (\n",
    "            metrics['rouge-1'] * 0.2 + metrics['rouge-2'] * 0.4 +\n",
    "            metrics['rouge-l'] * 0.4\n",
    "    )\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def validate(epoch, tokenizer, model, device, loader, max_length, ema):\n",
    "\n",
    "    \"\"\"\n",
    "    用于验证的方法：输入用于验证的数据，返回模型预测的结果和正确的标签\n",
    "    Function to evaluate model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ema.apply_shadow()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    total_metrics = {k: 0.0 for k in metric_keys}\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=max_length, \n",
    "              num_beams=2,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True\n",
    "              )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            \n",
    "            for i in range(len(preds)):\n",
    "                metrics = compute_metrics(preds[i], target[i])\n",
    "                for k, v in metrics.items():\n",
    "                    total_metrics[k] += v\n",
    "                count += 1\n",
    "                \n",
    "            print(\"preds: %s\\ntarget: %s\" % (preds[0], target[0]))\n",
    "                \n",
    "            if _ % 16 == 0:\n",
    "                console.print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "            \n",
    "    avg_metrics = {k: v / count for k, v in total_metrics.items()}\n",
    "                \n",
    "    print(avg_metrics)\n",
    "    console.log(f\"{avg_metrics}\\n\")\n",
    "    ema.restore()\n",
    "            \n",
    "    return predictions, actuals\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6c3da-081a-4707-8478-ad6444551014",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 训练类 Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9d4d302-61c8-41c8-a7ae-1074c14fffdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end...\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 训练类：整合数据集类、训练方法、验证方法，加载数据进行训练并验证训练过程的效果\n",
    "def T5Trainer(\n",
    "    dataframe, source_text, target_text, model_params, output_dir=\"./outputs/prompt/\"\n",
    "):\n",
    "    \"\"\"\n",
    "    T5 trainer\n",
    "    \"\"\"\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    \n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    # Defining the model. We are using PromptCLUE model and added a Language model layer on top for generation of prediction.\n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    # model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"], max_seq_len=model_params[\"MAX_SOURCE_TEXT_LENGTH\"])\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        # model = model.module.to(device)\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "    # Importing the raw dataset\n",
    "    dataframe = dataframe[[source_text, target_text]]\n",
    "    # display_df(dataframe.head(2))\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size So 94% of the data will be used for training and the rest for validation.\n",
    "    train_size = 0.9998\n",
    "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
    "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "    \n",
    "        \n",
    "    # 打印数据集相关日志：数据量、训练步数\n",
    "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    console.print(f\"VALID Dataset: {val_dataset.shape}\\n\")\n",
    "    total_train_steps = int((train_dataset.shape[0] * model_params[\"TRAIN_EPOCHS\"]) / model_params[\"TRAIN_BATCH_SIZE\"])\n",
    "    console.print(f\"Total Train Steps: {total_train_steps}\\n\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = SmallSampleDataSetClass(\n",
    "        train_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "    val_set = SmallSampleDataSetClass(\n",
    "        val_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    val_params = {\n",
    "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
    "    # optimizer = torch.optim.Adam(\n",
    "    #     params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
    "    # )\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), model_params[\"LEARNING_RATE\"])\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1 * total_train_steps, num_training_steps=total_train_steps)\n",
    "    \n",
    "    ema = EMA(model, 0.992)\n",
    "    ema.register()\n",
    "\n",
    "    # Training loop\n",
    "    console.log(f\"[Initiating Fine Tuning]...\\n\")\n",
    "\n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "        # 1) train for one epoch\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer, scheduler, ema)\n",
    "        \n",
    "        # 2) save model for each epoch\n",
    "        console.log(f\"[Saving Model]...\\n\")\n",
    "        path = os.path.join(output_dir, \"model_files\")\n",
    "        model.module.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        # 3) evaluating test dataset\n",
    "        console.log(f\"[Initiating Validation]...\\n\")\n",
    "        with torch.no_grad(): # add 2022.10.4\n",
    "            #for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "            predictions, actuals = validate(epoch, tokenizer, model.module, device, val_loader, model_params[\"MAX_TARGET_TEXT_LENGTH\"], ema)\n",
    "            final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
    "            final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"), encoding='utf8', index=None, sep=',')\n",
    "\n",
    "    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
    "\n",
    "    console.log(f\"[Validation Completed.]\\n\")\n",
    "    console.print(\n",
    "        f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\"\n",
    "    )\n",
    "    console.print(\n",
    "        f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n",
    "    )\n",
    "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e32c03e-7368-4808-8bd7-73260b388378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end...\n"
     ]
    }
   ],
   "source": [
    "# 定义模型的参数 let's define model parameters specific to T5\n",
    "model_params = {\n",
    "    \"MODEL\": \"outputs/prompt/trained_models/7/\",  # model_type pretrained_models/PromptCLUE-base & outputs/prompt/model_files/\n",
    "    \"TRAIN_BATCH_SIZE\": 12,  # training batch size, 8\n",
    "    \"VALID_BATCH_SIZE\": 16,  # validation batch size, 8 \n",
    "    \"TRAIN_EPOCHS\": 2,  # number of training epochs\n",
    "    \"VAL_EPOCHS\": 1,  # number of validation epochs\n",
    "    \"LEARNING_RATE\": 15e-5,  # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 500,  # max length of source text, 512\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 420,  # max length of target text,64\n",
    "    \"SEED\": 2022,  # set seed for reproducibility\n",
    "}\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264dec74-aa57-4a7d-80f2-3afaf50757a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.head:                                                input  \\\n",
      "0  根据标题和关键词生成文章：_标题：零壹智库|150份报告！把脉保险数字化及保险科技创新与发展...   \n",
      "1  根据标题和关键词生成文章：_标题：明晚直播|保险科技企业服务专题：保险营销服务赛道的机会与挑...   \n",
      "2  根据标题和关键词生成文章：_标题：专访熊猫保险科技创始人王刚：碎片化场景与精细化营销是保险科...   \n",
      "3  根据标题和关键词生成文章：_标题：熊猫保险科技出席2021保险科技创新发展论坛_关键词：4月...   \n",
      "4  根据标题和关键词生成文章：_标题：匠心深耕+科技驱动，众安引领保险科技行业新格局_关键词：新...   \n",
      "\n",
      "                                              target  \n",
      "0  科技与金融的融合正在加速。以大数据、人工智能、生物科技、区块链、物联网等为代表的技术不断成熟...  \n",
      "1  保险科技市场上，企业服务赛道正在成为新的主角：统计过去一年多的保险科技投融资案例，半数以上的...  \n",
      "2  日前，熊猫保险科技创始人、CEO王刚出席2021保险科技创新发展论坛暨保险科技管理人年会，并...  \n",
      "3  （4月16日讯）今日，熊猫保险科技受邀参加2021保险科技创新发展论坛暨保险科技管理人年会。...  \n",
      "4  去年，新冠疫情的袭来，给各行业带来巨大冲击，促使各行业加速洗牌。后疫情时代更是让处于下半场的...  \n",
      "df.shape: (160331, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09:29:10] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Loading outputs/prompt/trained_models/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>/<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                     <a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118005448.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py#19\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09:29:10]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Loading outputs/prompt/trained_models/\u001b[1;36m7\u001b[0m/\u001b[33m...\u001b[0m                                     \u001b]8;id=688566;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py\u001b\\\u001b[2m118005448.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=877467;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py#19\u001b\\\u001b[2m19\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                                         \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09:29:17] </span><span style=\"font-weight: bold\">[</span>Data<span style=\"font-weight: bold\">]</span>: Reading data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                                  <a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118005448.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py#35\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09:29:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mData\u001b[1m]\u001b[0m: Reading data\u001b[33m...\u001b[0m                                                                  \u001b]8;id=558307;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py\u001b\\\u001b[2m118005448.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=468461;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                                         \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">FULL Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">160331</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "FULL Dataset: \u001b[1m(\u001b[0m\u001b[1;36m160331\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TRAIN Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">160299</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TRAIN Dataset: \u001b[1m(\u001b[0m\u001b[1;36m160299\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">VALID Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "VALID Dataset: \u001b[1m(\u001b[0m\u001b[1;36m32\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total Train Steps: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26716</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total Train Steps: \u001b[1;36m26716\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09:29:19] </span><span style=\"font-weight: bold\">[</span>Initiating Fine Tuning<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                             <a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118005448.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py#112\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09:29:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Fine Tuning\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                                             \u001b]8;id=936692;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py\u001b\\\u001b[2m118005448.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=246522;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py#112\u001b\\\u001b[2m112\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                                        \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytorch13\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\anaconda3\\envs\\pytorch13\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100 epoch: 0; loss:2.2280; each step's time spent:1.26\n",
      "Step: 200 epoch: 0; loss:2.1167; each step's time spent:1.22\n",
      "Step: 300 epoch: 0; loss:2.1588; each step's time spent:1.20\n",
      "Step: 400 epoch: 0; loss:2.0052; each step's time spent:1.20\n",
      "Step: 500 epoch: 0; loss:1.7289; each step's time spent:1.19\n",
      "Step: 600 epoch: 0; loss:1.8018; each step's time spent:1.19\n",
      "Step: 700 epoch: 0; loss:1.7661; each step's time spent:1.19\n",
      "Step: 800 epoch: 0; loss:1.5520; each step's time spent:1.19\n",
      "Step: 900 epoch: 0; loss:1.7709; each step's time spent:1.19\n",
      "Step: 1000 epoch: 0; loss:1.4088; each step's time spent:1.19\n",
      "Step: 1100 epoch: 0; loss:1.5915; each step's time spent:1.19\n",
      "Step: 1200 epoch: 0; loss:1.7452; each step's time spent:1.19\n",
      "Step: 1300 epoch: 0; loss:1.8330; each step's time spent:1.19\n",
      "Step: 1400 epoch: 0; loss:1.5887; each step's time spent:1.19\n",
      "Step: 1500 epoch: 0; loss:1.7819; each step's time spent:1.19\n",
      "Step: 1600 epoch: 0; loss:1.5673; each step's time spent:1.19\n",
      "Step: 1700 epoch: 0; loss:1.7851; each step's time spent:1.19\n",
      "Step: 1800 epoch: 0; loss:1.4252; each step's time spent:1.19\n",
      "Step: 1900 epoch: 0; loss:1.6704; each step's time spent:1.19\n",
      "Step: 2000 epoch: 0; loss:1.5369; each step's time spent:1.19\n",
      "Step: 2100 epoch: 0; loss:1.6514; each step's time spent:1.18\n",
      "Step: 2200 epoch: 0; loss:1.6964; each step's time spent:1.18\n",
      "Step: 2300 epoch: 0; loss:1.5251; each step's time spent:1.18\n",
      "Step: 2400 epoch: 0; loss:1.5450; each step's time spent:1.18\n",
      "Step: 2500 epoch: 0; loss:1.6283; each step's time spent:1.18\n",
      "Step: 2600 epoch: 0; loss:1.4299; each step's time spent:1.18\n",
      "Step: 2700 epoch: 0; loss:1.6844; each step's time spent:1.18\n",
      "Step: 2800 epoch: 0; loss:1.7106; each step's time spent:1.18\n",
      "Step: 2900 epoch: 0; loss:1.3739; each step's time spent:1.18\n",
      "Step: 3000 epoch: 0; loss:1.6189; each step's time spent:1.18\n",
      "Step: 3100 epoch: 0; loss:1.3532; each step's time spent:1.18\n",
      "Step: 3200 epoch: 0; loss:1.5956; each step's time spent:1.18\n",
      "Step: 3300 epoch: 0; loss:1.5681; each step's time spent:1.18\n",
      "Step: 3400 epoch: 0; loss:1.5567; each step's time spent:1.18\n",
      "Step: 3500 epoch: 0; loss:1.2720; each step's time spent:1.18\n",
      "Step: 3600 epoch: 0; loss:1.5585; each step's time spent:1.18\n",
      "Step: 3700 epoch: 0; loss:1.5918; each step's time spent:1.18\n",
      "Step: 3800 epoch: 0; loss:1.4831; each step's time spent:1.18\n",
      "Step: 3900 epoch: 0; loss:1.4884; each step's time spent:1.18\n",
      "Step: 4000 epoch: 0; loss:1.7569; each step's time spent:1.18\n",
      "Step: 4100 epoch: 0; loss:1.7092; each step's time spent:1.18\n",
      "Step: 4200 epoch: 0; loss:1.6814; each step's time spent:1.18\n",
      "Step: 4300 epoch: 0; loss:1.5531; each step's time spent:1.18\n",
      "Step: 4400 epoch: 0; loss:1.5665; each step's time spent:1.18\n",
      "Step: 4500 epoch: 0; loss:1.6314; each step's time spent:1.18\n",
      "Step: 4600 epoch: 0; loss:1.9398; each step's time spent:1.18\n",
      "Step: 4700 epoch: 0; loss:1.7299; each step's time spent:1.18\n",
      "Step: 4800 epoch: 0; loss:1.5597; each step's time spent:1.18\n",
      "Step: 4900 epoch: 0; loss:1.2130; each step's time spent:1.18\n",
      "Step: 5000 epoch: 0; loss:1.7214; each step's time spent:1.18\n",
      "Step: 5100 epoch: 0; loss:1.8027; each step's time spent:1.18\n",
      "Step: 5200 epoch: 0; loss:1.2952; each step's time spent:1.18\n",
      "Step: 5300 epoch: 0; loss:1.8221; each step's time spent:1.18\n",
      "Step: 5400 epoch: 0; loss:1.2570; each step's time spent:1.18\n",
      "Step: 5500 epoch: 0; loss:1.8404; each step's time spent:1.18\n",
      "Step: 5600 epoch: 0; loss:1.6471; each step's time spent:1.18\n",
      "Step: 5700 epoch: 0; loss:1.4209; each step's time spent:1.18\n",
      "Step: 5800 epoch: 0; loss:1.3067; each step's time spent:1.18\n",
      "Step: 5900 epoch: 0; loss:1.6777; each step's time spent:1.18\n",
      "Step: 6000 epoch: 0; loss:1.9214; each step's time spent:1.18\n",
      "Step: 6100 epoch: 0; loss:1.7821; each step's time spent:1.18\n",
      "Step: 6200 epoch: 0; loss:1.6606; each step's time spent:1.18\n",
      "Step: 6300 epoch: 0; loss:1.5073; each step's time spent:1.18\n",
      "Step: 6400 epoch: 0; loss:1.4497; each step's time spent:1.18\n",
      "Step: 6500 epoch: 0; loss:1.4386; each step's time spent:1.18\n",
      "Step: 6600 epoch: 0; loss:1.9287; each step's time spent:1.18\n",
      "Step: 6700 epoch: 0; loss:1.6534; each step's time spent:1.18\n",
      "Step: 6800 epoch: 0; loss:1.3566; each step's time spent:1.18\n",
      "Step: 6900 epoch: 0; loss:1.8869; each step's time spent:1.18\n",
      "Step: 7000 epoch: 0; loss:1.9018; each step's time spent:1.18\n",
      "Step: 7100 epoch: 0; loss:1.6264; each step's time spent:1.18\n",
      "Step: 7200 epoch: 0; loss:1.8485; each step's time spent:1.18\n",
      "Step: 7300 epoch: 0; loss:1.6372; each step's time spent:1.18\n",
      "Step: 7400 epoch: 0; loss:1.6890; each step's time spent:1.18\n",
      "Step: 7500 epoch: 0; loss:2.0045; each step's time spent:1.18\n",
      "Step: 7600 epoch: 0; loss:1.9178; each step's time spent:1.18\n",
      "Step: 7700 epoch: 0; loss:1.7923; each step's time spent:1.18\n",
      "Step: 7800 epoch: 0; loss:1.7457; each step's time spent:1.18\n",
      "Step: 7900 epoch: 0; loss:1.8240; each step's time spent:1.18\n",
      "Step: 8000 epoch: 0; loss:1.7813; each step's time spent:1.18\n",
      "Step: 8100 epoch: 0; loss:1.7071; each step's time spent:1.18\n",
      "Step: 8200 epoch: 0; loss:1.3850; each step's time spent:1.18\n",
      "Step: 8300 epoch: 0; loss:1.7231; each step's time spent:1.18\n",
      "Step: 8400 epoch: 0; loss:2.0264; each step's time spent:1.18\n",
      "Step: 8500 epoch: 0; loss:1.7240; each step's time spent:1.18\n",
      "Step: 8600 epoch: 0; loss:1.6399; each step's time spent:1.18\n",
      "Step: 8700 epoch: 0; loss:1.6818; each step's time spent:1.18\n",
      "Step: 8800 epoch: 0; loss:1.5913; each step's time spent:1.18\n",
      "Step: 8900 epoch: 0; loss:1.8618; each step's time spent:1.18\n",
      "Step: 9000 epoch: 0; loss:1.6743; each step's time spent:1.18\n",
      "Step: 9100 epoch: 0; loss:1.7143; each step's time spent:1.18\n",
      "Step: 9200 epoch: 0; loss:1.5240; each step's time spent:1.18\n",
      "Step: 9300 epoch: 0; loss:1.9498; each step's time spent:1.18\n",
      "Step: 9400 epoch: 0; loss:1.9672; each step's time spent:1.18\n",
      "Step: 9500 epoch: 0; loss:1.5094; each step's time spent:1.18\n",
      "Step: 9600 epoch: 0; loss:1.8657; each step's time spent:1.18\n",
      "Step: 9700 epoch: 0; loss:1.3689; each step's time spent:1.18\n",
      "Step: 9800 epoch: 0; loss:1.6625; each step's time spent:1.18\n",
      "Step: 9900 epoch: 0; loss:1.4019; each step's time spent:1.18\n",
      "Step: 10000 epoch: 0; loss:1.6879; each step's time spent:1.18\n",
      "Step: 10100 epoch: 0; loss:2.1264; each step's time spent:1.18\n",
      "Step: 10200 epoch: 0; loss:1.5936; each step's time spent:1.18\n",
      "Step: 10300 epoch: 0; loss:1.6789; each step's time spent:1.18\n",
      "Step: 10400 epoch: 0; loss:1.8971; each step's time spent:1.18\n",
      "Step: 10500 epoch: 0; loss:1.7589; each step's time spent:1.18\n",
      "Step: 10600 epoch: 0; loss:1.9652; each step's time spent:1.18\n",
      "Step: 10700 epoch: 0; loss:2.1338; each step's time spent:1.18\n",
      "Step: 10800 epoch: 0; loss:1.7650; each step's time spent:1.18\n",
      "Step: 10900 epoch: 0; loss:1.9613; each step's time spent:1.18\n",
      "Step: 11000 epoch: 0; loss:1.6481; each step's time spent:1.18\n",
      "Step: 11100 epoch: 0; loss:2.0177; each step's time spent:1.18\n",
      "Step: 11200 epoch: 0; loss:1.9437; each step's time spent:1.18\n",
      "Step: 11300 epoch: 0; loss:1.9195; each step's time spent:1.18\n",
      "Step: 11400 epoch: 0; loss:1.8576; each step's time spent:1.18\n",
      "Step: 11500 epoch: 0; loss:1.8752; each step's time spent:1.18\n",
      "Step: 11600 epoch: 0; loss:1.9980; each step's time spent:1.18\n",
      "Step: 11700 epoch: 0; loss:1.7420; each step's time spent:1.18\n",
      "Step: 11800 epoch: 0; loss:1.9096; each step's time spent:1.18\n",
      "Step: 11900 epoch: 0; loss:1.8731; each step's time spent:1.18\n",
      "Step: 12000 epoch: 0; loss:1.6722; each step's time spent:1.18\n",
      "Step: 12100 epoch: 0; loss:1.5841; each step's time spent:1.18\n",
      "Step: 12200 epoch: 0; loss:1.5926; each step's time spent:1.18\n",
      "Step: 12300 epoch: 0; loss:2.0178; each step's time spent:1.18\n",
      "Step: 12400 epoch: 0; loss:1.8924; each step's time spent:1.18\n",
      "Step: 12500 epoch: 0; loss:1.8313; each step's time spent:1.18\n",
      "Step: 12600 epoch: 0; loss:1.8346; each step's time spent:1.18\n",
      "Step: 12700 epoch: 0; loss:1.8550; each step's time spent:1.18\n",
      "Step: 12800 epoch: 0; loss:1.9115; each step's time spent:1.18\n",
      "Step: 12900 epoch: 0; loss:2.0007; each step's time spent:1.18\n",
      "Step: 13000 epoch: 0; loss:1.7744; each step's time spent:1.18\n",
      "Step: 13100 epoch: 0; loss:2.2075; each step's time spent:1.18\n",
      "Step: 13200 epoch: 0; loss:2.0847; each step's time spent:1.18\n",
      "Step: 13300 epoch: 0; loss:1.6775; each step's time spent:1.18\n",
      "Step: 13358 epoch: 0; loss:1.4323; each step's time spent:1.18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[13:52:19] </span><span style=\"font-weight: bold\">[</span>Saving Model<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                                       <a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118005448.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py#119\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">119</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[13:52:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mSaving Model\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                                                       \u001b]8;id=30109;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py\u001b\\\u001b[2m118005448.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=813711;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py#119\u001b\\\u001b[2m119\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                                        \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[13:52:24] </span><span style=\"font-weight: bold\">[</span>Initiating Validation<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                              <a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118005448.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py#126\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[13:52:24]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Validation\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                                              \u001b]8;id=75147;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py\u001b\\\u001b[2m118005448.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=207984;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\118005448.py#126\u001b\\\u001b[2m126\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                                        \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: 5月20日，助力科技强省建设全省科技创新大会在合肥举行。副省长肖菊华出席并讲话。他强调，要深入贯彻落实全省科技创新大会精神，大力促进科技与金融深度“联姻”，让科技创新“不差钱”科技人才“不愁钱”。肖菊华指出，科技强省建设必须以科技金融为支撑，要以项目签约为新起点，推动形成多元化、多层次、多渠道的科技创新投入体系，努力实现“纸变钱”。肖菊华强调，科技部门要积极协同金融、财政、金融监管部门加快科技成果转化，提档升级，推动政府引导基金市场化、专业化、规范化发展;科技研发金融机构要按照科技强省任务，加大对科技企业需求和市场规律的分析研判，优化金融产品、服务、知识产权质押融资、科技保险等科技金融产品，不断深化“纸变钱”改革，打造更多资本催化剂，更好发挥科技赋能作用。\n",
      "target: 5月20日，科技金融服务“滴灌行动”助力科技强省建设重大项目签约活动在汉举行。副省长肖菊华出席，强调深入贯彻落实全省科技创新大会精神，大力促进科技和金融深度“联姻”，让科技创新“不差钱”、科技人才“不愁钱”。肖菊华指出，科技强省建设，科技金融不可或缺、大有作为。要以此次项目签约为新起点，推动形成多元化、多层次、多渠道的科技创新投入体系，加快“纸变钱”“钱生钱”。肖菊华强调，科技部门要积极协同金融、财政等部门，推进科技金融服务“滴灌行动”提档升级，推动政府引导基金市场化、专业化、规范化发展，激励和引导更多社会资金投入高新技术行业和科技中小企业，打造全产业链科技金融服务体系。金融监管部门和金融机构要根据科技强省任务、科技企业需求和市场规律，优化金融产品和服务，大力发展知识产权质押融资、科技保险等科技金融产品，为科技成果“纸变钱”注入更多“资本催化剂”。\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: 近日，黄浦区金光外滩中心的一处融商务、居住、餐饮、娱乐于一体的现代化建筑群正式对外出租。该栋写字楼位于黄浦区金光外滩中心1号楼2层，建筑面积为2001.5平方米，容积率为2002.10，地上27层，裙房、机电、机电设备房、机电、空调等120间，交付标准为A+级;标准交付的住宅有：生物医药、芯片半导体、人工智能科技等。个人介绍：金光外滩中心由金融行业、互联网行业、科技行业、大数据行业、服务型行业等企业创立，选址在金光外滩中心3号楼5层，计划预算约20亿元。\n",
      "target: 金光外滩中心总投资约四亿美元，是上海外滩城区唯一一处融商务、居住、餐饮和娱乐等多功能的现代化建筑群，总建筑面积约19万平方米。由波特曼建筑设计事务所负责设计。2001.5~2002.10新东京负责酒店(27层)，裙房，地下室的机电深化设计及其它各区的机电设备房(共约8万平方米)的机电深化设计。租赁面积：120平方--2521平方整层租金：6--8.5元/平方/天(不含税)交付标准：精装修及标准交付【适合业态】：总部办公、生物医药研发、芯片半导体研发，新能源检测，人工智能科技等【个人介绍】本人从事地产行业多年，服务过各行各业公司，金融行业，互联网行业，科技行业，大数据行业，服务型行业等等，从企业创立之初为其选址，计划预算，到后期扩大，提供等等一系列服务。\n",
      "{'main': 0.5587540076503827, 'rouge-1': 0.6828565129558951, 'rouge-2': 0.44857827693980235, 'rouge-l': 0.6068784857082066}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[13:54:17] </span><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'main'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5587540076503827</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rouge-1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6828565129558951</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rouge-2'</span>:                   <a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\132358616.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">132358616.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\132358616.py#85\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">85</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.44857827693980235</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rouge-l'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6068784857082066</span><span style=\"font-weight: bold\">}</span>                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[13:54:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m{\u001b[0m\u001b[32m'main'\u001b[0m: \u001b[1;36m0.5587540076503827\u001b[0m, \u001b[32m'rouge-1'\u001b[0m: \u001b[1;36m0.6828565129558951\u001b[0m, \u001b[32m'rouge-2'\u001b[0m:                   \u001b]8;id=267822;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\132358616.py\u001b\\\u001b[2m132358616.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=250491;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18976\\132358616.py#85\u001b\\\u001b[2m85\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36m0.44857827693980235\u001b[0m, \u001b[32m'rouge-l'\u001b[0m: \u001b[1;36m0.6068784857082066\u001b[0m\u001b[1m}\u001b[0m                                      \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                                         \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100 epoch: 1; loss:1.3052; each step's time spent:1.22\n",
      "Step: 200 epoch: 1; loss:1.6905; each step's time spent:1.20\n",
      "Step: 300 epoch: 1; loss:2.0328; each step's time spent:1.20\n",
      "Step: 400 epoch: 1; loss:1.8595; each step's time spent:1.19\n",
      "Step: 500 epoch: 1; loss:2.0246; each step's time spent:1.19\n",
      "Step: 600 epoch: 1; loss:1.6146; each step's time spent:1.19\n",
      "Step: 700 epoch: 1; loss:1.7234; each step's time spent:1.19\n",
      "Step: 800 epoch: 1; loss:1.8933; each step's time spent:1.19\n",
      "Step: 900 epoch: 1; loss:1.8595; each step's time spent:1.19\n",
      "Step: 1000 epoch: 1; loss:2.2911; each step's time spent:1.19\n",
      "Step: 1100 epoch: 1; loss:1.8001; each step's time spent:1.19\n",
      "Step: 1200 epoch: 1; loss:1.7406; each step's time spent:1.19\n",
      "Step: 1300 epoch: 1; loss:1.6552; each step's time spent:1.19\n",
      "Step: 1400 epoch: 1; loss:1.7111; each step's time spent:1.19\n",
      "Step: 1500 epoch: 1; loss:1.8312; each step's time spent:1.19\n",
      "Step: 1600 epoch: 1; loss:1.6639; each step's time spent:1.18\n",
      "Step: 1700 epoch: 1; loss:1.5280; each step's time spent:1.18\n",
      "Step: 1800 epoch: 1; loss:1.9601; each step's time spent:1.18\n",
      "Step: 1900 epoch: 1; loss:1.6913; each step's time spent:1.18\n",
      "Step: 2000 epoch: 1; loss:1.7493; each step's time spent:1.18\n",
      "Step: 2100 epoch: 1; loss:1.8989; each step's time spent:1.18\n",
      "Step: 2200 epoch: 1; loss:1.3774; each step's time spent:1.18\n",
      "Step: 2300 epoch: 1; loss:1.6648; each step's time spent:1.18\n",
      "Step: 2400 epoch: 1; loss:1.8957; each step's time spent:1.18\n",
      "Step: 2500 epoch: 1; loss:1.8781; each step's time spent:1.18\n",
      "Step: 2600 epoch: 1; loss:1.6083; each step's time spent:1.18\n",
      "Step: 2700 epoch: 1; loss:1.8313; each step's time spent:1.18\n",
      "Step: 2800 epoch: 1; loss:1.7890; each step's time spent:1.18\n",
      "Step: 2900 epoch: 1; loss:1.6043; each step's time spent:1.18\n",
      "Step: 3000 epoch: 1; loss:1.9137; each step's time spent:1.18\n",
      "Step: 3100 epoch: 1; loss:2.2691; each step's time spent:1.18\n",
      "Step: 3200 epoch: 1; loss:1.7580; each step's time spent:1.18\n",
      "Step: 3300 epoch: 1; loss:1.6166; each step's time spent:1.18\n",
      "Step: 3400 epoch: 1; loss:1.7720; each step's time spent:1.18\n",
      "Step: 3500 epoch: 1; loss:1.9191; each step's time spent:1.18\n",
      "Step: 3600 epoch: 1; loss:1.8391; each step's time spent:1.18\n",
      "Step: 3700 epoch: 1; loss:1.9944; each step's time spent:1.18\n",
      "Step: 3800 epoch: 1; loss:2.2124; each step's time spent:1.18\n",
      "Step: 3900 epoch: 1; loss:1.9580; each step's time spent:1.18\n",
      "Step: 4000 epoch: 1; loss:1.5162; each step's time spent:1.18\n",
      "Step: 4100 epoch: 1; loss:1.3839; each step's time spent:1.18\n",
      "Step: 4200 epoch: 1; loss:2.0725; each step's time spent:1.18\n",
      "Step: 4300 epoch: 1; loss:1.8299; each step's time spent:1.18\n",
      "Step: 4400 epoch: 1; loss:1.8716; each step's time spent:1.18\n",
      "Step: 4500 epoch: 1; loss:1.7892; each step's time spent:1.18\n",
      "Step: 4600 epoch: 1; loss:1.9073; each step's time spent:1.18\n",
      "Step: 4700 epoch: 1; loss:2.2617; each step's time spent:1.18\n",
      "Step: 4800 epoch: 1; loss:2.1254; each step's time spent:1.18\n",
      "Step: 4900 epoch: 1; loss:1.5447; each step's time spent:1.18\n",
      "Step: 5000 epoch: 1; loss:1.5772; each step's time spent:1.19\n",
      "Step: 5100 epoch: 1; loss:1.5336; each step's time spent:1.19\n",
      "Step: 5200 epoch: 1; loss:1.7301; each step's time spent:1.19\n",
      "Step: 5300 epoch: 1; loss:1.6121; each step's time spent:1.19\n",
      "Step: 5400 epoch: 1; loss:2.1872; each step's time spent:1.19\n",
      "Step: 5500 epoch: 1; loss:1.5993; each step's time spent:1.19\n",
      "Step: 5600 epoch: 1; loss:1.8256; each step's time spent:1.19\n",
      "Step: 5700 epoch: 1; loss:1.7612; each step's time spent:1.19\n",
      "Step: 5800 epoch: 1; loss:1.7353; each step's time spent:1.19\n",
      "Step: 5900 epoch: 1; loss:1.7830; each step's time spent:1.19\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "# 使用 pCLUE:1200000+多任务提示学习数据集 的部分数据\n",
    "# dataframe必须有2列: \n",
    "#   - input: 文本输入\n",
    "#   - target: 目标输出\n",
    "df = pd.read_csv('data/prompt/train_prompt.tsv', sep='\\t', encoding='utf8')  # 数据量：1200k数据。\n",
    "# df = df.sample(frac=0.01) # TODO  取消本行代码，如果你需要更多数据训练\n",
    "print(\"df.head:\",df.head(n=5))\n",
    "print(\"df.shape:\",df.shape)\n",
    "# 显存占用说明：如果运行现在显存不足，请使用nvidia-smi查看显存；如果显卡多数被占用了，请重启colab程序\n",
    "T5Trainer(\n",
    "    dataframe=df,\n",
    "    source_text=\"input\",\n",
    "    target_text=\"target\",\n",
    "    model_params=model_params,\n",
    "    output_dir=\"outputs/prompt/\"\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"end..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec9b9e-97ae-4542-a326-a3aee5cccd52",
   "metadata": {},
   "source": [
    "# 预测 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "153d4253-be9f-4cba-8e32-c59b438ae120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end...\n"
     ]
    }
   ],
   "source": [
    "def testing(tokenizer, model, device, loader, max_length):\n",
    "\n",
    "    \"\"\"\n",
    "    用于预测的方法：输入用于预测的数据，返回模型预测的结果\n",
    "    Function for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length=max_length, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            \n",
    "            print(_, preds[0])\n",
    "            if _ % 100 == 0:\n",
    "                console.print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "    return predictions\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db1772d9-e5b0-4926-9614-12e9329dfe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end...\n"
     ]
    }
   ],
   "source": [
    "# 训练类：整合数据集类、训练方法、验证方法，加载数据进行训练并验证训练过程的效果\n",
    "def T5Tester(\n",
    "    dataframe, source_text, model_params, output_dir=\"./outputs/prompt/\"\n",
    "):\n",
    "    \"\"\"\n",
    "    T5 tester\n",
    "    \"\"\"\n",
    "    # n_gpu = torch.cuda.device_count()\n",
    "    n_gpu = 1\n",
    "    \n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(model_params[\"SEED\"])\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    # Defining the model. We are using PromptCLUE model and added a Language model layer on top for generation of prediction.\n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        # model = model.module.cuda()\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "    # Importing the raw dataset\n",
    "    # dataframe = dataframe[source_text]\n",
    "    # display_df(dataframe.head(2))\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size So 94% of the data will be used for training and the rest for validation.\n",
    "    test_dataset = dataframe\n",
    "    \n",
    "    # 打印数据集相关日志：数据量、训练步数\n",
    "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TEST Dataset: {test_dataset.shape}\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    testing_set = SmallSampleDataSetClass(\n",
    "        test_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text=source_text,\n",
    "        target_text=None\n",
    "    )\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    test_params = {\n",
    "        \"batch_size\": model_params[\"TEST_BATCH_SIZE\"],\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0\n",
    "    }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    testing_loader = DataLoader(testing_set, **test_params)\n",
    "    \n",
    "    # 3) evaluating test dataset\n",
    "    console.log(f\"[Initiating Prediction]...\\n\")\n",
    "    with torch.no_grad(): # add 2022.10.4\n",
    "        #for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "        predictions = testing(tokenizer, model, device, testing_loader, model_params[\"MAX_TARGET_TEXT_LENGTH\"])\n",
    "        final_df = pd.DataFrame({\"Generated Text\": predictions})\n",
    "        final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"), encoding='utf8', index=None)\n",
    "\n",
    "    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
    "\n",
    "    console.log(f\"[Prediction Completed.]\\n\")\n",
    "    console.print(\n",
    "        f\"\"\"[Prediction] Generation on Testing data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n",
    "    )\n",
    "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca3f1a35-b379-49b7-96aa-36cd3ce2bd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end...\n"
     ]
    }
   ],
   "source": [
    "# 定义模型的参数 let's define model parameters specific to T5\n",
    "model_params = {\n",
    "    \"MODEL\": \"outputs/prompt/model_files/\",  # model_type\n",
    "    \"TEST_BATCH_SIZE\": 32,  # training batch size, 8\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 330,  # max length of source text, 512\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 420,  # max length of target text,64\n",
    "    \"SEED\": 2022,  # set seed for reproducibility\n",
    "}\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccc9dc41-12c5-485f-a31e-135f19eb4b42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.head:                                                input\n",
      "0  根据标题和关键词生成文章：_标题：统计局：7月份，制造业采购经理指数(PMI)为49.0%_...\n",
      "1  根据标题和关键词生成文章：_标题：统计局：7月份制造业采购经理指数为49.0%_关键词：统计...\n",
      "2  根据标题和关键词生成文章：_标题：货币政策这十年：管好总闸门的勇气与探索_关键词：作者;杨志...\n",
      "3  根据标题和关键词生成文章：_标题：央行：通过结构性货币政策工具等多种政策引导金融机构精准支持...\n",
      "4  根据标题和关键词生成文章：_标题：7月份中国采购经理指数公布：新动能保持上升势头就业保持相对...\n",
      "df.shape: (1209, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[23:32:08] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Loading outputs/prompt/model_files/<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                          <a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">877665802.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py#20\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[23:32:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Loading outputs/prompt/model_files/\u001b[33m...\u001b[0m                                          \u001b]8;id=649829;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py\u001b\\\u001b[2m877665802.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=688650;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py#20\u001b\\\u001b[2m20\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                                         \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[23:32:15] </span><span style=\"font-weight: bold\">[</span>Data<span style=\"font-weight: bold\">]</span>: Reading data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                                  <a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">877665802.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py#35\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[23:32:15]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mData\u001b[1m]\u001b[0m: Reading data\u001b[33m...\u001b[0m                                                                  \u001b]8;id=715946;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py\u001b\\\u001b[2m877665802.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=17387;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                                         \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">FULL Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1209</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "FULL Dataset: \u001b[1m(\u001b[0m\u001b[1;36m1209\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TEST Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1209</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TEST Dataset: \u001b[1m(\u001b[0m\u001b[1;36m1209\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Initiating Prediction<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                               <a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">877665802.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py#70\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">70</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Prediction\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                                               \u001b]8;id=930049;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py\u001b\\\u001b[2m877665802.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=991354;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py#70\u001b\\\u001b[2m70\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                                         \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 据国家统计局网站31日消息，7月份，制造业采购经理指数(PMI)为49.0%，比上月下降1.2个百分点，继续低于临界点。从企业规模看，大型企业PMI为48.5%，比上月下降0.4个百分点，降至临界点以下;中型企业PMI为49.5%，比上月下降0.7个百分点，降至临界点以下。从分类指数看，构成制造业PMI的三个分类指数分别为：供应商配送时间指数高于临界点、新订单指数和新订单指数。生产指数为48.5%，比上月下降1.1个百分点，降至临界点以下。新订单指数为48.5%，比上月下降1.9个百分点，降至临界点以下。\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 习近平在中央统战工作会议上强调促进海内外中华儿女团结奋斗为中华民族伟大复兴汇聚伟力7月29日至30日，中共中央总书记、国家主席、中央军委主席习近平在中央统战工作会议上发表重要讲话。他强调，要坚持以习近平新时代中国特色社会主义思想为指导，坚持中国共产党领导，坚持中国特色社会主义道路和社会主义伟大旗帜，统一思想认识，坚持围绕中心、服务大局，坚定不移推进党和国家事业发展，以优异成绩庆祝建党100周年，推动实现中华民族伟大复兴。习近平指出，党的十八大以来，全国上下始终把人民生命财产安全放在首位，坚持稳中求进工作总基调，全面贯彻新发展理念，不断增强政治定力，努力开创各项工作新局面。\n",
      "2 据国家统计局网站31日消息，7月份，制造业采购经理指数(PMI)为49.0%，比上月下降1.2个百分点，制造业总体景气水平有所回落。从企业规模看，大型企业PMI为49.5%，比上月下降2.2个百分点，降至临界点以下;中、小型企业PMI分别为47.5%和41.9%，比上月下降0.7个百分点，降至临界点以下。从分类指数看，构成制造业PMI的5个分类指数分别为生产指数、新订单指数、新订单指数和新订单指数。生产指数为49.0%，比上月下降3.0个百分点，降至临界点以下。原材料库存指数为48.4%，比上月下降1.9个百分点，降至临界点以下。新订单指数为48.4%，比上月下降1.9个百分点，降至临界点以下。\n",
      "3 本文来自：国家统计局7月31日，国家统计局服务业调查中心高级统计师赵庆河解读表示，7月份，制造业采购经理指数(PMI)为49。比上月下降1.2个百分点，位于临界点以下;非制造业商务活动指数为48。综合PMI为48。比上月下降0.9、1.6个百分点，位于临界点以下。总体上看，经济景气水平有所回落，但恢复基础尚需进一步夯实。7月官方制造业PMI为49据统计局数据显示，7月份，制造业采购经理指数为49。比上月下降1.2个百分点，位于临界点以下。从企业规模看，中型企业PMI为49.8%，比上月下降0.4、2.8个百分点，重回临界点以下。生产指数为47.9%，比上月下降0.7个百分点，重回临界点以下。\n",
      "4 新华社北京6月30日电(王沪宁出席并讲话)新华社电中国国家版本馆开馆暨展览开幕式30日上午在北京举行。中共中央政治局常委、中央书记处书记王沪宁发表讲话，宣布中国国家版本馆正式开馆。王沪宁对党的十八大以来中华文化传承发展作出一系列重要指示，阐明新时代传承发展中华文化的一系列方向性和根本性，强调要深入贯彻习近平新时代中国特色社会主义思想，深刻领悟“两个确立”、坚定“四个自信”、做到“两个维护”，牢记文化使命，赓续中华民族伟大复兴的伟业。王沪宁指出，中国国家版本馆将立足新气象、新阵地，展示中国形象，为世界科普中国贡献中国智慧。研究中心将发挥“史鉴之源”作用，为后人更好地了解和传承中华优秀传统文化提供参考。\n",
      "5 今天是中国共产党成立100周年。从石库门到天安门，走过百年风雨征程，迎来101周岁的生日。党的十八大以来，广大党员和干部以实际行动庆祝建党100周年，他们有着48年党龄，在“七一”前夕来临之际，重温入党誓词。历久弥坚，有信仰、有力量、有担当。初心坚定，出生成长，深受红色家风洗礼。习近平总书记说：“上中学时，我学了政治课本叫《共产党人》，讲了生产劳动的艰辛。”一代人对理想信念的坚守，始终如一。\n",
      "6 文/观察者网王慧编辑/冯雪在经济下行压力下，面对通胀的冲击，美联储开启了上世纪80年代的加息周期。6月加息75个基点，以2.5%的涨幅领跑全球。在美联储带头加息的情况下，美联储连续多次加息，世界范围内掀起了新一轮的通货膨胀浪潮。欧洲央行也迈出历史性一步，为11年来首次加息。这让各国国家收紧货币政策的“推手”感到不安，困扰全球的通胀问题愈发凸显。从历史性的视角来看，美国已经率先退出量化宽松政策。\n",
      "7 本文源自:金融界网金融界网7月21日消息，今日A股三大指数震荡回落，其中沪指小幅收跌0.01%，深成指、创业板指跌幅均在2.5%以上，符合市场预期，两市成交额连续3个交易日突破万亿元。7月21日国常会强调，鼓励和支持实体经济发展。对于经济刺激，还有进一步的打压。美联储加息对经济刺激政策的影响是非常大的，所以加息幅度理应是相当大的。如果货币政策不收紧，那么利率就会上升。有可能会持续下去。\n",
      "8 习近平在庆祝中国人民解放军建军90周年大会上的讲话新华社记者鞠鹏摄8月1日出版的《中国日报》杂志正式发表中共中央总书记、国家主席、中央军委主席习近平的重要讲话。大会主要内容为庆祝中国人民解放军建军90周年。2017年8月1日至2021年3月31日，习近平在庆祝中国人民解放军建军90周年大会上发表重要讲话。新华社记者鞠鹏摄2017年8月1日至2021年3月31日，习近平在庆祝中国人民解放军建军90周年大会上发表重要讲话。新华社记者鞠鹏摄习近平总书记指出，中国共产党领导中华民族伟大复兴和人民军队伟大复兴，展望国防和军队现代化建设的光明前景，动员全党全军全国各族人民团结奋斗，汇聚起强国强军的磅礴力量，实现了“两个一百年”奋斗目标，实现了中华民族伟大复兴和中华民族伟大复兴。\n",
      "9 周末财经要闻汇总2021-07-26来源：东方财富研究中心作者：佚名关注中金在线：扫描二维码关注√中金在线微信在线咨询：扫描或点击关注中金在线客服证监会部署下半年七大重点任务证监会：支持刚性公开上市证监会部署上半年证券市场监管工作6月25日，沪深交易所分别发布《股票上市新规》和《上海证券交易所创业板股票上市规则》。根据交易细则，自2021年7月1日起，上交所将对已上市的科创板股票实施退市风险警示。同时，上交所还发布了《关于进一步优化营商环境促进中小企业健康发展的若干意见》，并将于2021年7月1日起正式实施。此外，证监会还特别限制了开展房地产养老储蓄试点工作、完善房地产消费若干措施等。“十四五”期间，国家房产基金公司总经理丁文武接受审查调查，因新冠病毒检测结果不合格，证监会决定终止其发行上市审核。\n",
      "10 新华财经北京7月1日电(王菁)债市：基本面扰动渐弱期债主力持续冲高回落，现券表现较弱，多数期限收益率下行，市场情绪有所回暖。央行公布最新一期中期借贷便利(MLF)利率决议，提前交易对市场影响有限。观点称，上修MLF将不会再次上演“阶段性”行情。市场对疫后疫情的确认与预期一致，但市场博弈仍可能会切换至通胀和利率上下波动区间。此外，随着疫情防控政策的逐步落实，短期债券价格或有上行空间。\n",
      "11 受疫情影响，食品企业和慈善机构面临压力。据美国媒体30日报道，由于通胀增加，美国依赖食品银行领取免费食物的慈善机构正面临压力。食品银行是一种为人们发放免费食品的金融工具，但目前仍面临40%的通胀压力。食品银行的援助需求激增，对食品和供应链的冲击正在加剧。据报道，美国爱达荷州博伊西市一家食品银行日前发布的报告显示，该机构在过去一周中为4200人提供食物的人数增加了50%，而亚利桑那州的一家慈善机构则减少了78%。根据这份报告，该机构最近一周为4271个家庭提供了78次免费食物。\n",
      "12 作者：全球治理中心(GlobalCentralLabor)刊于7月26日，《中国经济发展报告》正式发布。在新发展格局下，我国经济面临前所未有的挑战。在统一大势、实现高质量发展的时候，我们认为，经济积累了很多风险，但从这个观点来看，应该坚持和发展为先，把两个大局下我国经济发展理念定位起来。首先，必须处好历史阶段两者之间的关键性平衡点。一是改革开放的先行者;二是世界百年未有之大变局的转折点。我们要坚定不移地推进改革创新，加快建设社会主义现代化国家。要继续深化改革开放，以更大决心和更实举措推动经济社会高质量发展。第二个大局迫使我国在内外部环境发生深刻变化，完成经济长期稳定恢复。\n",
      "13 黄金自4月份以来表现震荡，截至2021年3月31日，该指数累计上涨29.23%，为连续第7个月下跌。基金经理4年的业绩分析研判显示，当前市场整体呈现震荡态势。Wind数据显示，5月份PMI略有回升，2070.42%有300个基点，其中6月份有100个基点。从目前来看，黄金价格会出现调整，尤其是近期逆市布局的“避险之王”黄金，价值多于预期。中国基金报记者专访了七大基金经理，他们分别是：中证指数投资部总经理助理、中证指数量化投资部高级总监张博、中证黄金私募基金金融工程与投资部副总经理朱金钰、中证ETF基金经理周天明、中证ETF基金经理周海涛、中证ETF基金经理周建华、中证ETF基金经理李永强、中证ETF指数投资中心投资(总监钱晶)。\n",
      "14 专业术语：TZA，是新兴的中市值指标，也是一支综合性的指数。它专门涵盖有长期增长潜力的成份指数和股票指数。目前，TZA在沪深两市总股数2000家，其中沪市500家，深市300家。从2000年初至今，TZA一直存在做空交易，即借入股票卖出的一种反向操作方式。TZA是罗素2000指数的衍生产品，移动、联通、电信等应该都持有该类股票。因此，如果我们把TZA作为一种反向基金的话，那么TZA就有可能成为新的市场参与者。\n",
      "15 投资、决策，来A股参谋部吧!点击进入超话>>>王广华主持召开自然资源部党组会议传达学习习近平总书记重要指示精神研究部署生态环境工作6月30日，自然资源部党组书记、局长王广华主持召开自然资源部党组会议。传达学习习近平总书记在四川考察调研的重要指示精神，研究部署生态环境工作。王广华指出，地质灾害防治和生态文明建设是关系国计民生的重大工程。生态环境部是生态文明建设的主责部门，是党中央、国务院决策部署的重要组成部分。部系统各级党组织和全体党员干部必须高度重视生态环境工作，坚定不移贯彻新发展理念，按照新发展理念、新任务、新要求，坚持人与自然和谐共生，认真落实好《中共中央关于全面深化改革若干重大问题的决定》等重要论述，以实际行动迎接党的二十大胜利召开。\n",
      "16 香港回归祖国届满25周年，香港回归后取得新成就。一个25年，走向何方？经济腾飞，资本和技术不断涌现。企业引入“内地+香港”的桥梁，为内地经济的发展注入了新的活力。但与此同时，也遭遇百年变局，科技创新力不足、累积的人才优势不强。在新时代巨潮中占据潮头的，应坚持稳中求进工作总基调，以创新驱动发展，以科技赋能产业高质量发展，以科技发展为市场，助力建设新发展格局，加快构建新发展格局。\n",
      "17 7月29日，西城集团党委书记、董事长齐向东在“全球数字经济大会数字经济产业园区发展论坛”上表示，网络安全运营存在巨大挑战。要增强数字城市网络安全能力，以“零事故”为目标，实现“零事故”中国方案的落地。他透露，当前我国网络安全面临的挑战和挑战正在不断增加。目前，我国网络安全运营水平已经达到世界先进水平。近日，《2022中国数字城市网络安全运营现状分析报告》发布，全国范围的城市中，有6个城市已建成了网络安全运营中心。其中，北京、上海、广州等城市网络安全运营成熟度较高，但仍暂无城市能够达到或超过其专业安全水平。\n",
      "18 本文转自：重庆日报(实习生)记者雍黎6月30日，重庆青创训练营暨2022重庆市科技和金融路演中心揭牌仪式在重庆举行。作为一项议程，本次学员投融资路演的举办，意味着对企业家高管参训6月28日至30日期间，重庆市40名科技和金融路演中心展开为期3天的培训活动。此次培训由重庆市科技局、重庆团市委指导，重庆银保监局、中国银行保险监督管理委员会、重庆银保监局、重庆证监局、重庆金控集团主办，重庆创投创新创业服务中心、重庆科技金融服务中心有限公司等协办。据了解，本次训练营以“理论教学+实操教学”相结合，围绕“国际前沿科技企业制定发展战略”、“科创企业与营销”、“科创企业上市筹备”三大主题进行授课。\n",
      "19 周末主要有一条利空消息上周五大盘和股市都上涨了。消息面上，今天大盘应该小幅高开，上攻时关注3270点，压力在3290点附近，回落时关注3250点，支撑在3580点附近。消息面上，中国化学(601398)二腈项目投产突破产业链关键核心技术，四川路桥将计划委托比亚迪、中能矿产、铁锂项目进行经营管理，南岭环保拟53.57亿元购买易普力95.54%的股份，拟收购江西纬科100%股权看好未来胶膜行业发展，九安医疗等美国子公司已履行完毕，中伟股份、森邦科技净利8142万元，预计2021年一季度净利润同比增长13%。\n",
      "20 粤港澳大湾区金融合作创新平台(下称“粤港澳大湾区金融通”)近日在广东深圳发布，为粤港澳大湾区发展描绘了广阔前景，大湾区金融迎来了新的发展机遇。据《中国经营报》记者了解，粤港澳大湾区金融通的发布三年多以来，广东省地方金融监管局会同有关部门，出台细化、规范的金融改革举措，推动粤港澳金融合作不断深化。很多改革措施都是开创性的，包括跨境金融互联互通机制、横琴港深港通、债券通、跨境理财通、境外金融通等。目前，粤港澳大湾区已在全国范围内率先实现跨境金融业务全范围的开放，金融产品实现多项全国首创，金融科技和跨境金融正加速落地。\n",
      "21 有伟大祖国的坚强领导，有伟大的人民的大力支持，有亿万人民的保障，有改革开放的宏伟蓝图。在第二个百年奋斗目标新征程上，我们能够创造更加美好的未来，更能够为香港发展作出更大贡献。6月25日，庆祝香港回归祖国25周年大会暨香港特别行政区第六届政府(行政长官)会在北京开幕。习近平主席发表重要讲话，对香港充满期待和信心。会场外，习主席通过视频连线的方式，从视野系统回顾了“一国”制度创新、战略思考、行动指南等，充分彰显了中国坚定不移走中国特色社会主义道路的决心和意志，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心，坚定信心。\n",
      "22 本文源自:格隆汇财经早餐美股方面，美股三大指数集体收涨。道指涨0.4%，标普收涨1.42%，纳指涨1.88%。苹果、微软和亚马逊的亮眼财报带动了市场情绪。雪佛龙财报也带动能源板块大涨。工业板块涨2%，原材料板块涨1.3%，医疗板块收跌0.3%。苹果涨3.3%，苹果公司第三财季净利润为4亿美元，较上年同期增长19%。Facebook涨11.4%，营收为2亿美元。谷歌涨1.6%，市值重回前10。谷歌涨1.8%，市值重回前10。据彭博社报道，媒体消息称，对冲基金正在减持一些股票，以帮助投资者降低风险。\n",
      "23 久期财经客户端第一时间接收最全面的市场资讯，获取最新最全的财经资讯!下载地址：新浪财经APP【日本政府年金投资基金】日前披露年报，确认该基金出现重大变化。作为全球管理资产规模最大的公募基金之一，日本政府年金投资基金(简称“年金投资基金”)于2021财年第二季度以196万亿日元(约合人民币21.18亿元)的份额获得超额收益。考虑到日本从财年到今年二季度的四分法进行资产配置，即长期配置本土股票、外国股票和债券等四类资产，因此未能在二级市场中持续稳定增长。不过，这并不意味着日本政府年金投资基金就出现较大程度的亏损。\n",
      "24 沙洲村的红色传递《求是》杂志记者陈有勇摄新湘评论(记者周亚明湖南省汝城县沙洲村)属于“半条被子”。1934年11月，红军长征途经沙洲村时，3名女红军借宿于此。村民徐解秀家里有一床被子剪下一半，另一条被子分两半，用绳子捆着。2020年9月16日，习近平总书记来到湖南考察，参观了沙洲村专题陈列馆，了解当地群众生活情况。总书记强调，要切实把人民群众的困难和问题解决好，为解放思想、建设社会主义现代化新湖南贡献智慧和力量。他指出：“我们党始终把人民生命财产安全放在首位，全心全意为人民服务。”党的十八大以来，中国共产党人团结带领中国人民艰苦奋斗、艰苦奋斗，在艰苦卓绝的生活中不断发展壮大。\n",
      "25 今天我们看到，了解到总书记非常关心台湾的问题。罗鼎钧：习近平总书记的信，对台湾的友好和友善，客观上我觉得很有意义。大陆的台湾同胞，有义务跟他们做个人的事。我谨遵总书记的教诲，在新媒体社交帐号、社交媒体平台等平台上，不断向台湾人传播中华文化。从去年开始，我先后到两岸对岸交流，求学了十多年。几年来，我吸引了近十万粉丝关注，从线上扩展到线下，为台湾十几所高校、近千名台学生提供学习交流平台。目前，已举办过十几场大陆就学、百名台湾青年参与组织的活动。\n",
      "26 作者：扎埃夫(保加利亚)“一带一路”建设是世界性重大问题。扎埃夫教授特别喜欢用政治活动来分析中国的经济社会，他将召开中共二十大。中国人民最关心的是什么大事？制度如何充分体现？人民选举出什么样的结果？讨论关乎未来一年国家经济社会发展的方方面面？全过程的人民民主令人对即将召开的中共二十大充满希望。全文如下：充分体现了人民民主的内涵和深刻反映了中国共产党在2022年开启的百年征程中取得的重大成就。\n",
      "27 香港《南华早报》网站7月25日发表题为《面对美国施压，中国毫不慌张》的文章。文章指出，以美国为首的西方国家在全球秩序上不断加大了对中国的制裁。十个理由让我们感到不安：没有“美国”;没有“美国”;没有“中国”。这两条战线已经打了一场全球战争，俄罗斯、乌克兰和俄罗斯赌注，而民主国家正在崛起，中国并没有因此受到威胁。这是为什么呢？有这样的理由：绝大多数国家都对美国抱有怀疑态度。但中国却丝毫不慌张。\n",
      "28 总结五年发展成绩，上海锚定未来五年发展目标。“十三五”期间，参照系和金名片的上海经济高质量发展目标已落到实处。位于黄浦江畔的上海世博中心作党代会报告时表示，将积极融入全球经济体系，积极参与全球经济治理，为上海建设社会主义现代化国际大都市提供有力支撑。上海指数：“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，“十三五”期间，上海方案将成为国际规则制定、参照系和参照系，上海创新、上海品牌将成为享誉全球的现代城市。上海市第十二次党代会(图片来源：党代会报告)数据显示，2020年，上海市全市生产总值达到4.32万亿元，金融市场交易总额突破2500万亿元，同比增长8.3%。\n",
      "29 ，为深入实施新时代人才强军战略作出更大贡献。近日，中共中央、国务院印发《关于加强人才队伍建设的若干意见》，强调“要扎实推进人才工作，以中央人才工作会议精神为指导，做好中央军委人才工作会议各项工作”。这篇文章就从如何深入实施新时代人才强军战略、发挥人才重要引领作用出发，提出建议。纵观百年党史，中国人民军队坚持党的领导，坚定不移贯彻新发展理念，健全制度、机制、机制，维护国家主权和安全稳定，统一思想、凝聚力量，在革命、建设、发展中不断取得新成就。在新发展的战略机遇期，中国人才事业正向着更高水平的跃升。为维护地区和平稳定提供强大力量支撑，中国人才事业正朝着更加高质量发展迈进。\n",
      "30 2030年前实现“双碳”目标，中国制造业何为？在历史中，从碳达峰到碳中和，是中国乃至全世界许下的宏伟目标。在中国工程院院士李强看来，人类生活可能存在碳足迹，但目前仍占据着重要地位。从原材料采购、加工、处理、产品制造、使用，到产品消费、废弃物处理等全生命周期，工业领域都面临诸多挑战。相关数据显示，2020年我国二氧化碳排放70年来首次突破1亿吨大关。而工业也是制造业的主战场，实现“双碳”目标的路径有待探索。如何实现“双碳”目标，是关键问题。首先，要加快构建绿色低碳发展体系。其次，要强化产业链供应链协同联动。第三，要完善生产流程、精细化控制，提高效率。\n",
      "31 原标题：拿到“教资”只是成为在编教师第一步，后续三个步骤，每个都很难前几年，报考教资人数不断增加，尤其是疫情影响下，考教资是考教师编制第一步的关键。但凡学生有成为在编教师的意愿，那么“教资”就一定要拥有好编制工作，多选择教资的人，打算一毕业就从事教师行业，希望考上教资，让教资证书和学位证都能用上。然而，随着教师行业成为了退路，想成为老师又想考取编制的教育机构，部分同学却想要通过考试。\n",
      "32 坐拥35平方公里的黄金口岸，是长沙面积最大的城区之一，具有得天独厚的城市拓展空间和区位优势。时值盛夏，“触目惊心党”处处是大干项目，火热场面背后是党员干部大干实干的生动写照。为填补长沙产业链空白，望城区以强链补链项目为抓手，助力长沙打造千亿级5G智能终端产业园内的第一个签约入驻、智慧物联终端制造基地等项目，抢滩“一江之滨”。近年来，望城人民医院、大泽湖海归小镇建设有序推进，在一江之畔，群众幸福感获得感显著提升。\n",
      "33 不知道是哪一天，今天宣布了解散的消息。昨天凌晨，美股市场又疯传了一家独角兽公司，还有一段疑似在线上会议的优消息，但随后震荡，28日美股盘前听了一遍，结果发现录音里全程都显示“倒闭”。字眼：这是一家独角兽公司。之前宣布的大部分员工的工作截止到7月28日，因为裁员比例太大，所以不得不提前通知管理层。大目标：新业务：达华智慧菜场、零售云、阿里云、腾讯云等。这个方案已经不是什么新鲜事。\n",
      "34 原标题：每日优鲜不行，非赖前置仓？图片来源：来咖智库作者：柴犬7月28日，深交所召开2021年第4次审议会议。会议录音了多名员工的发言。公司表示，为实现盈利的大目标，公司对前置仓业务进行了调整，并明确了“每日达”的业务调整，部分员工离职，可能被辞退。会议录音中，有多位员工向媒体爆料，未来将逐步回归前置仓市场。从上述消息扩散开来，大家都知道，7月29日，又有多位员工到北京市朝阳区，倒下让市场颇感意外。上海、北京疫情期间，生鲜电商如顺丰、马鲜生、美团买菜等纷纷宣布撤回上市申请。但对于公司业务来说，却有成为“保供者”的趋势。\n",
      "35 今天(7月30日)，中国国家版本馆开馆。中央总馆文瀚阁、西安分馆文济阁、杭州分馆文润阁、上海分馆文崇德阁等“一总三分”建设国家版本馆。党中央批准实施的《中国国家版本馆总体规划》提出，中国国家版本馆是国家版本资源总库、中华文化种子基因库、历史文化传承价值高地。建设目标是赓续和展示大国形象，把中华民族伟大复兴作为写进“十四五”规划的重要内容。这对于建设国家版本馆具有重大意义，也对推动构建新发展格局、建设社会主义现代化国家大都市具有重要意义。国家版本馆：传国家版本馆位于北京大兴燕山北麓，是北京地标性建筑。该库房由北京市文物局设计，采用中国传统建筑风格，韵味十足，建筑落成后可追溯到元代。\n",
      "36 普洱茶，始于雍正年间，由鄂尔泰出任云贵总督的自治县设立普洱府，并命名为“茶茅”。它承担着茶叶加工和流通的职能。普洱茶在宁洱困鹿山、古滇高原、古云南高原、古西藏高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南高原、古云南中西部、古云南中西部、古云南中西部、古云南中东部、古云南中西部、古云南中西部、古云南中西部、古云南中北部、古云南中西部、古云南中西部、古云南中西部、古云南中西部、古云南中西部、古云南中南部、古云南中西部、古云南中西部、古云南中西部、古云南中西部、古云南中西部、古云南中西部、古云南中西部、古云南中西部、古云南中西部、古云南中西部、古云南中西部、古云南中西部地区。其中，以神农为代表。而神农是茶树的发源地。自古以来就是皇家贡茶的所在地。唐朝时期，乾隆、嘉庆、道光三个朝代相继发展，形成了一个庞大的民族群。从内地迁入至今，带来了\n",
      "37 澎湃新闻记者陈灿杰实习生李梦雅编辑黄霁洁农妇韦巧“我要上班了，洗碗、洗衣服。”54岁的丈夫抱怨装碗、摞床。6月25日，他骑着电瓶车，赶去离家近12公里外的农村。“三千块的工资到账，我这识字，会让丈夫很郁闷。”“房子首付太高，5点就来不及了。”丈夫王启三去工厂接妻子下班时，口吐白沫，说：“你媳妇现在太热了，要加班了。”随后，她又进入家中休息。据国家气候中心监测，6月1日至7月12日，我国平均气温为39.2°C，高温日数为1961年以来的最高值。\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[00:05:28] </span><span style=\"font-weight: bold\">[</span>Prediction Completed.<span style=\"font-weight: bold\">]</span>                                                                  <a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">877665802.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py#79\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">79</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[00:05:28]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mPrediction Completed.\u001b[1m]\u001b[0m                                                                  \u001b]8;id=405267;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py\u001b\\\u001b[2m877665802.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=950185;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\877665802.py#79\u001b\\\u001b[2m79\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                                         \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Prediction<span style=\"font-weight: bold\">]</span> Generation on Testing data saved @ outputs/prompt/prediction/predictions.csv\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mPrediction\u001b[1m]\u001b[0m Generation on Testing data saved @ outputs/prompt/prediction/predictions.csv\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Logs<span style=\"font-weight: bold\">]</span> Logs saved @ outputs/prompt/prediction/logs.txt\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mLogs\u001b[1m]\u001b[0m Logs saved @ outputs/prompt/prediction/logs.txt\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end..\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "# 使用 pCLUE:1200000+多任务提示学习数据集 的部分数据\n",
    "# dataframe必须有2列: \n",
    "#   - input: 文本输入\n",
    "#   - target: 目标输出\n",
    "df = pd.read_csv('data/prompt/testB_prompt.tsv', sep='\\t', encoding='utf8', header=0, names=[\"input\"])  # 数据量：1675数据。\n",
    "# df = df.sample(frac=0.01) # TODO  取消本行代码，如果你需要更多数据训练\n",
    "print(\"df.head:\",df.head(n=5))\n",
    "print(\"df.shape:\",df.shape)\n",
    "# 显存占用说明：如果运行现在显存不足，请使用nvidia-smi查看显存；如果显卡多数被占用了，请重启colab程序\n",
    "T5Tester(\n",
    "    dataframe=df,\n",
    "    source_text=\"input\",\n",
    "    model_params=model_params,\n",
    "    output_dir=\"outputs/prompt/prediction/\",\n",
    ")\n",
    "torch.cuda.empty_cache() \n",
    "print(\"end..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f370954d-2cf5-4657-87f6-76bb45863340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看训练后显存占用情况。如果显存被占用，可以kill掉相关的进程\n",
    "!nvidia-smi\n",
    "# !fuser -v /dev/nvidia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998828ad-ad79-4f9e-b414-919edc986962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi -r \n",
    "# 使用以下命令清除训练中残存的GPU显存缓存\n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839dd22-938e-4f30-9623-0f1d020cd270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
