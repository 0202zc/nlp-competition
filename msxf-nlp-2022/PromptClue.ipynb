{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40cbef5-1289-42ed-97f5-28fe9377e6e5",
   "metadata": {},
   "source": [
    "# 使用自定义数据集训练PromptCLUE模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f521b0-e13d-4b4b-bb9b-064b78607439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end2...\n"
     ]
    }
   ],
   "source": [
    "# 引入相应的包 Importing libraries\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import time, json\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "# from models.modeling_t5 import T5ForConditionalGeneration\n",
    "\n",
    "# rich: for a better display on terminal\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "print(\"end2...\")\n",
    "\n",
    "# 计算rouge用\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7aab58-3d96-41f1-ae77-cfa9edf71ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end...\n"
     ]
    }
   ],
   "source": [
    "# 做一些相关的配置(打印显示；GPU设置)\n",
    "# define a rich console logger\n",
    "console = Console(record=True)\n",
    "\n",
    "# to display dataframe in ASCII format\n",
    "def display_df(df):\n",
    "    \"\"\"display dataframe in ASCII format\"\"\"\n",
    "\n",
    "    console = Console()\n",
    "    table = Table(\n",
    "        Column(\"source_text\", justify=\"center\"),\n",
    "        Column(\"target_text\", justify=\"center\"),\n",
    "        title=\"Sample Data\",\n",
    "        pad_edge=False,\n",
    "        box=box.ASCII,\n",
    "    )\n",
    "\n",
    "    for i, row in enumerate(df.values.tolist()):\n",
    "        table.add_row(row[0], row[1])\n",
    "\n",
    "    # console.print(table) # TODO TODO TODO \n",
    "\n",
    "# training logger to log training progress\n",
    "training_logger = Table(\n",
    "    Column(\"Epoch\", justify=\"center\"),\n",
    "    Column(\"Steps\", justify=\"center\"),\n",
    "    Column(\"Loss\", justify=\"center\"),\n",
    "    title=\"Training Status\",\n",
    "    pad_edge=False,\n",
    "    box=box.ASCII,\n",
    ")\n",
    "\n",
    "# Setting up the device for GPU usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d6c66-f468-43b9-8c03-d72f5594248b",
   "metadata": {},
   "source": [
    "# Dataset Class 自定义数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1040b801-29b9-4a4f-a31f-60fe11a85d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end...\n"
     ]
    }
   ],
   "source": [
    "class SmallSampleDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    创建一个自定义的数据集，用于训练，必须包括两个字段：输入(如source_text)、输出（如target_text）\n",
    "    Creating a custom dataset for reading the dataset and\n",
    "    loading it into the dataloader to pass it to the\n",
    "    neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Dataset class\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): Input dataframe\n",
    "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
    "            source_len (int): Max length of source text\n",
    "            target_len (int): Max length of target text\n",
    "            source_text (str): column name of source text\n",
    "            target_text (str): column name of target text\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text] if target_text is not None else None\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns the length of dataframe\"\"\"\n",
    "        if self.target_text is not None:\n",
    "            return len(self.target_text)\n",
    "        else:\n",
    "            return len(self.source_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if self.target_text is not None:\n",
    "            \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
    "\n",
    "            source_text = str(self.source_text[index])\n",
    "            target_text = str(self.target_text[index])\n",
    "\n",
    "            # cleaning data so as to ensure data is in string type\n",
    "            source_text = \" \".join(source_text.split())\n",
    "            target_text = \" \".join(target_text.split())\n",
    "\n",
    "            source = self.tokenizer.batch_encode_plus(\n",
    "                [source_text],\n",
    "                max_length=self.source_len,\n",
    "                pad_to_max_length=True,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            target = self.tokenizer.batch_encode_plus(\n",
    "                [target_text],\n",
    "                max_length=self.summ_len,\n",
    "                pad_to_max_length=True,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            source_ids = source[\"input_ids\"].squeeze()\n",
    "            source_mask = source[\"attention_mask\"].squeeze()\n",
    "            target_ids = target[\"input_ids\"].squeeze()\n",
    "            target_mask = target[\"attention_mask\"].squeeze()\n",
    "\n",
    "            return {\n",
    "                \"source_ids\": source_ids.to(dtype=torch.long),\n",
    "                \"source_mask\": source_mask.to(dtype=torch.long),\n",
    "                \"target_ids\": target_ids.to(dtype=torch.long),\n",
    "                \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
    "            }\n",
    "        else:\n",
    "            \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
    "\n",
    "            source_text = str(self.source_text[index])\n",
    "\n",
    "            # cleaning data so as to ensure data is in string type\n",
    "            source_text = \" \".join(source_text.split())\n",
    "\n",
    "            source = self.tokenizer.batch_encode_plus(\n",
    "                [source_text],\n",
    "                max_length=self.source_len,\n",
    "                pad_to_max_length=True,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            source_ids = source[\"input_ids\"].squeeze()\n",
    "            source_mask = source[\"attention_mask\"].squeeze()\n",
    "\n",
    "            return {\n",
    "                \"source_ids\": source_ids.to(dtype=torch.long),\n",
    "                \"source_mask\": source_mask.to(dtype=torch.long)\n",
    "            }\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb44c17-202a-46e2-86c2-0f301ead32b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 训练方法 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a69058b5-fe86-418f-a5b7-e12198436fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db736ef5-357f-4587-9ef2-d543b287c48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end...\n"
     ]
    }
   ],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer, scheduler, ema):\n",
    "\n",
    "    \"\"\"\n",
    "    用于训练的方法\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "\n",
    "    model.train()\n",
    "    time1 = time.time()\n",
    "    for _, data in enumerate(loader, 0):\n",
    "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
    "        y_ids = y[:, :-1].contiguous() # target, from start to end(except end of token, <EOS>). e.g. \"你好吗？\"\n",
    "        lm_labels = y[:, 1:].clone().detach() # target, for second to end.e.g.\"好吗？<EOS>\"\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100 # releted to pad_token and loss. for detail, check here: https://github.com/Shivanandroy/T5-Finetuning-PyTorch/issues/3\n",
    "        ids = data[\"source_ids\"].to(device, dtype=torch.long) # input. e.g. \"how are you?\"\n",
    "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            decoder_input_ids=y_ids,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "            \n",
    "        # 每100步打印日志\n",
    "        if _ % 100 == 0 and _!=0:\n",
    "            time2 = time.time()\n",
    "            print(\"Step:\", _,\"epoch: \" + str(epoch) + \"; loss:{:.4f}; each step's time spent:{:.2f}\".format(loss.detach().cpu().numpy(), float(time2-time1) / float(_ + 0.0001)))\n",
    "            # training_logger.add_row(str(epoch), str(_), str(loss))\n",
    "            # console.print(training_logger)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ema.update()\n",
    "        scheduler.step()\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a96d01b-d310-45e6-a505-40ff1055a83a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 用于验证的方法 Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "425d1bb4-f139-4bdc-99da-700d04d70ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end...\n"
     ]
    }
   ],
   "source": [
    "metric_keys = ['main', 'rouge-1', 'rouge-2', 'rouge-l']\n",
    "\n",
    "def compute_rouge(source, target, unit='word'):\n",
    "    \"\"\"计算rouge-1、rouge-2、rouge-l\n",
    "    \"\"\"\n",
    "    # if unit == 'word':\n",
    "    #     source = jieba.cut(source, HMM=False)\n",
    "    #     target = jieba.cut(target, HMM=False)\n",
    "    source, target = ' '.join(source), ' '.join(target)\n",
    "    try:\n",
    "        scores = rouge.get_scores(hyps=source, refs=target)\n",
    "        return {\n",
    "            'rouge-1': scores[0]['rouge-1']['f'],\n",
    "            'rouge-2': scores[0]['rouge-2']['f'],\n",
    "            'rouge-l': scores[0]['rouge-l']['f'],\n",
    "        }\n",
    "    except ValueError:\n",
    "        return {\n",
    "            'rouge-1': 0.0,\n",
    "            'rouge-2': 0.0,\n",
    "            'rouge-l': 0.0,\n",
    "        }\n",
    "\n",
    "    \n",
    "def compute_metrics(source, target, unit='word'):\n",
    "    \"\"\"计算所有metrics\n",
    "    \"\"\"\n",
    "    metrics = compute_rouge(source, target, unit)\n",
    "    metrics['main'] = (\n",
    "            metrics['rouge-1'] * 0.2 + metrics['rouge-2'] * 0.4 +\n",
    "            metrics['rouge-l'] * 0.4\n",
    "    )\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def validate(epoch, tokenizer, model, device, loader, max_length, ema):\n",
    "\n",
    "    \"\"\"\n",
    "    用于验证的方法：输入用于验证的数据，返回模型预测的结果和正确的标签\n",
    "    Function to evaluate model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ema.apply_shadow()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    total_metrics = {k: 0.0 for k in metric_keys}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=max_length, \n",
    "              num_beams=2,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True\n",
    "              )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            \n",
    "            for i in range(len(preds)):\n",
    "                metrics = compute_metrics(preds[i], target[i])\n",
    "                for k, v in metrics.items():\n",
    "                    total_metrics[k] += v\n",
    "                \n",
    "            print(\"preds: %s\\ntarget: %s\" % (preds[0], target[0]))\n",
    "                \n",
    "            if _ % 100 == 0:\n",
    "                console.print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "            \n",
    "    avg_metrics = {k: v / len(loader) for k, v in total_metrics.items()}\n",
    "                \n",
    "    print(avg_metrics)\n",
    "    ema.restore()\n",
    "            \n",
    "    return predictions, actuals\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6c3da-081a-4707-8478-ad6444551014",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 训练类 Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9d4d302-61c8-41c8-a7ae-1074c14fffdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end...\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 训练类：整合数据集类、训练方法、验证方法，加载数据进行训练并验证训练过程的效果\n",
    "def T5Trainer(\n",
    "    dataframe, source_text, target_text, model_params, output_dir=\"./outputs/prompt/\"\n",
    "):\n",
    "    \"\"\"\n",
    "    T5 trainer\n",
    "    \"\"\"\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    \n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    # Defining the model. We are using PromptCLUE model and added a Language model layer on top for generation of prediction.\n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    # model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"], max_seq_len=model_params[\"MAX_SOURCE_TEXT_LENGTH\"])\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        # model = model.module.to(device)\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "    # Importing the raw dataset\n",
    "    dataframe = dataframe[[source_text, target_text]]\n",
    "    # display_df(dataframe.head(2))\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size So 94% of the data will be used for training and the rest for validation.\n",
    "    train_size = 0.9998\n",
    "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
    "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "    \n",
    "    # 分层抽样\n",
    "#     ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=model_params[\"SEED\"])\n",
    "#     strat_train_set = []\n",
    "#     strat_test_set = []\n",
    "    \n",
    "#     for train_index, test_index in ss.split(dataframe, dataframe['target']):\n",
    "#         strat_train_set = dataframe.iloc[train_index, :]\n",
    "#         strat_test_set = dataframe.iloc[test_index, :]\n",
    "        \n",
    "#     train_dataset = strat_train_set.reset_index(drop=True)\n",
    "#     val_dataset = strat_test_set.reset_index(drop=True)\n",
    "        \n",
    "    # 打印数据集相关日志：数据量、训练步数\n",
    "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    console.print(f\"VALID Dataset: {val_dataset.shape}\\n\")\n",
    "    total_train_steps = int((train_dataset.shape[0] * model_params[\"TRAIN_EPOCHS\"]) / model_params[\"TRAIN_BATCH_SIZE\"])\n",
    "    console.print(f\"Total Train Steps: {total_train_steps}\\n\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = SmallSampleDataSetClass(\n",
    "        train_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "    val_set = SmallSampleDataSetClass(\n",
    "        val_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    val_params = {\n",
    "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
    "    # optimizer = torch.optim.Adam(\n",
    "    #     params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
    "    # )\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), model_params[\"LEARNING_RATE\"])\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1 * total_train_steps, num_training_steps=total_train_steps)\n",
    "    \n",
    "    ema = EMA(model, 0.99)\n",
    "    ema.register()\n",
    "\n",
    "    # Training loop\n",
    "    console.log(f\"[Initiating Fine Tuning]...\\n\")\n",
    "\n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "        # 1) train for one epoch\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer, scheduler, ema)\n",
    "        \n",
    "        # 2) save model for each epoch\n",
    "        console.log(f\"[Saving Model]...\\n\")\n",
    "        path = os.path.join(output_dir, \"model_files\")\n",
    "        model.module.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        # 3) evaluating test dataset\n",
    "        console.log(f\"[Initiating Validation]...\\n\")\n",
    "        with torch.no_grad(): # add 2022.10.4\n",
    "            #for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "            predictions, actuals = validate(epoch, tokenizer, model.module, device, val_loader, model_params[\"MAX_TARGET_TEXT_LENGTH\"], ema)\n",
    "            final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
    "            final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"), encoding='utf8', index=None, sep=',')\n",
    "\n",
    "    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
    "\n",
    "    console.log(f\"[Validation Completed.]\\n\")\n",
    "    console.print(\n",
    "        f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\"\n",
    "    )\n",
    "    console.print(\n",
    "        f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n",
    "    )\n",
    "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e32c03e-7368-4808-8bd7-73260b388378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end...\n"
     ]
    }
   ],
   "source": [
    "# 定义模型的参数 let's define model parameters specific to T5\n",
    "model_params = {\n",
    "    \"MODEL\": \"outputs/prompt/trained_models/5/\",  # model_type pretrained_models/PromptCLUE-base & outputs/prompt/model_files/\n",
    "    \"TRAIN_BATCH_SIZE\": 12,  # training batch size, 8\n",
    "    \"VALID_BATCH_SIZE\": 16,  # validation batch size, 8 \n",
    "    \"TRAIN_EPOCHS\": 1,  # number of training epochs\n",
    "    \"VAL_EPOCHS\": 1,  # number of validation epochs\n",
    "    \"LEARNING_RATE\": 1e-4,  # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 500,  # max length of source text, 512\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 420,  # max length of target text,64\n",
    "    \"SEED\": 2022,  # set seed for reproducibility\n",
    "}\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264dec74-aa57-4a7d-80f2-3afaf50757a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.head:                                                input  \\\n",
      "0  根据标题和关键词生成文章：_标题：零壹智库|150份报告！把脉保险数字化及保险科技创新与发展...   \n",
      "1  根据标题和关键词生成文章：_标题：明晚直播|保险科技企业服务专题：保险营销服务赛道的机会与挑...   \n",
      "2  根据标题和关键词生成文章：_标题：专访熊猫保险科技创始人王刚：碎片化场景与精细化营销是保险科...   \n",
      "3  根据标题和关键词生成文章：_标题：熊猫保险科技出席2021保险科技创新发展论坛_关键词：4月...   \n",
      "4  根据标题和关键词生成文章：_标题：匠心深耕+科技驱动，众安引领保险科技行业新格局_关键词：新...   \n",
      "\n",
      "                                              target  \n",
      "0  科技与金融的融合正在加速。以大数据、人工智能、生物科技、区块链、物联网等为代表的技术不断成熟...  \n",
      "1  保险科技市场上，企业服务赛道正在成为新的主角：统计过去一年多的保险科技投融资案例，半数以上的...  \n",
      "2  日前，熊猫保险科技创始人、CEO王刚出席2021保险科技创新发展论坛暨保险科技管理人年会，并...  \n",
      "3  （4月16日讯）今日，熊猫保险科技受邀参加2021保险科技创新发展论坛暨保险科技管理人年会。...  \n",
      "4  去年，新冠疫情的袭来，给各行业带来巨大冲击，促使各行业加速洗牌。后疫情时代更是让处于下半场的...  \n",
      "df.shape: (160331, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[19:07:09] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Loading outputs/prompt/trained_models/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>/<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                    <a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\3835596664.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3835596664.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\3835596664.py#19\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[19:07:09]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Loading outputs/prompt/trained_models/\u001b[1;36m5\u001b[0m/\u001b[33m...\u001b[0m                                    \u001b]8;id=518532;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\3835596664.py\u001b\\\u001b[2m3835596664.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=578838;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\3835596664.py#19\u001b\\\u001b[2m19\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                                        \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[19:07:16] </span><span style=\"font-weight: bold\">[</span>Data<span style=\"font-weight: bold\">]</span>: Reading data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                                 <a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\3835596664.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3835596664.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\3835596664.py#35\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[19:07:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mData\u001b[1m]\u001b[0m: Reading data\u001b[33m...\u001b[0m                                                                 \u001b]8;id=520759;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\3835596664.py\u001b\\\u001b[2m3835596664.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=185277;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\3835596664.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                                        \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">FULL Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">160331</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "FULL Dataset: \u001b[1m(\u001b[0m\u001b[1;36m160331\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TRAIN Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">160299</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TRAIN Dataset: \u001b[1m(\u001b[0m\u001b[1;36m160299\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">VALID Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "VALID Dataset: \u001b[1m(\u001b[0m\u001b[1;36m32\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total Train Steps: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13358</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total Train Steps: \u001b[1;36m13358\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[19:07:19] </span><span style=\"font-weight: bold\">[</span>Initiating Fine Tuning<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                            <a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\3835596664.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3835596664.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\3835596664.py#112\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[19:07:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Fine Tuning\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                                            \u001b]8;id=231861;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\3835596664.py\u001b\\\u001b[2m3835596664.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=786781;file://C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12056\\3835596664.py#112\u001b\\\u001b[2m112\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                                       \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytorch13\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\anaconda3\\envs\\pytorch13\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100 epoch: 0; loss:2.3165; each step's time spent:1.26\n",
      "Step: 200 epoch: 0; loss:2.1876; each step's time spent:1.22\n",
      "Step: 300 epoch: 0; loss:2.2671; each step's time spent:1.20\n",
      "Step: 400 epoch: 0; loss:2.1188; each step's time spent:1.20\n",
      "Step: 500 epoch: 0; loss:1.8480; each step's time spent:1.20\n",
      "Step: 600 epoch: 0; loss:1.9495; each step's time spent:1.19\n",
      "Step: 700 epoch: 0; loss:1.9336; each step's time spent:1.19\n",
      "Step: 800 epoch: 0; loss:1.7241; each step's time spent:1.19\n",
      "Step: 900 epoch: 0; loss:1.9506; each step's time spent:1.19\n",
      "Step: 1000 epoch: 0; loss:1.5986; each step's time spent:1.19\n",
      "Step: 1100 epoch: 0; loss:1.8282; each step's time spent:1.19\n",
      "Step: 1200 epoch: 0; loss:1.9906; each step's time spent:1.19\n",
      "Step: 1300 epoch: 0; loss:2.0864; each step's time spent:1.19\n",
      "Step: 1400 epoch: 0; loss:1.8500; each step's time spent:1.19\n",
      "Step: 1500 epoch: 0; loss:2.0828; each step's time spent:1.19\n",
      "Step: 1600 epoch: 0; loss:1.8728; each step's time spent:1.19\n",
      "Step: 1700 epoch: 0; loss:2.0646; each step's time spent:1.19\n",
      "Step: 1800 epoch: 0; loss:1.8173; each step's time spent:1.19\n",
      "Step: 1900 epoch: 0; loss:2.0230; each step's time spent:1.19\n",
      "Step: 2000 epoch: 0; loss:1.9062; each step's time spent:1.19\n",
      "Step: 2100 epoch: 0; loss:1.9787; each step's time spent:1.19\n",
      "Step: 2200 epoch: 0; loss:2.0116; each step's time spent:1.19\n",
      "Step: 2300 epoch: 0; loss:1.8400; each step's time spent:1.18\n",
      "Step: 2400 epoch: 0; loss:1.8481; each step's time spent:1.18\n",
      "Step: 2500 epoch: 0; loss:1.9645; each step's time spent:1.18\n",
      "Step: 2600 epoch: 0; loss:1.7505; each step's time spent:1.18\n",
      "Step: 2700 epoch: 0; loss:2.0785; each step's time spent:1.18\n",
      "Step: 2800 epoch: 0; loss:2.0473; each step's time spent:1.18\n",
      "Step: 2900 epoch: 0; loss:1.6804; each step's time spent:1.18\n",
      "Step: 3000 epoch: 0; loss:1.9128; each step's time spent:1.18\n",
      "Step: 3100 epoch: 0; loss:1.6779; each step's time spent:1.18\n",
      "Step: 3200 epoch: 0; loss:1.9419; each step's time spent:1.18\n",
      "Step: 3300 epoch: 0; loss:1.8661; each step's time spent:1.18\n",
      "Step: 3400 epoch: 0; loss:1.8706; each step's time spent:1.18\n",
      "Step: 3500 epoch: 0; loss:1.5340; each step's time spent:1.18\n",
      "Step: 3600 epoch: 0; loss:1.8206; each step's time spent:1.18\n",
      "Step: 3700 epoch: 0; loss:1.9328; each step's time spent:1.18\n",
      "Step: 3800 epoch: 0; loss:1.7666; each step's time spent:1.18\n",
      "Step: 3900 epoch: 0; loss:1.7913; each step's time spent:1.18\n",
      "Step: 4000 epoch: 0; loss:2.0807; each step's time spent:1.18\n",
      "Step: 4100 epoch: 0; loss:2.0277; each step's time spent:1.18\n",
      "Step: 4200 epoch: 0; loss:1.9671; each step's time spent:1.18\n",
      "Step: 4300 epoch: 0; loss:1.7731; each step's time spent:1.18\n",
      "Step: 4400 epoch: 0; loss:1.8331; each step's time spent:1.18\n",
      "Step: 4500 epoch: 0; loss:1.8386; each step's time spent:1.18\n",
      "Step: 4600 epoch: 0; loss:2.2461; each step's time spent:1.18\n",
      "Step: 4700 epoch: 0; loss:2.0068; each step's time spent:1.18\n",
      "Step: 4800 epoch: 0; loss:1.8429; each step's time spent:1.18\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "# 使用 pCLUE:1200000+多任务提示学习数据集 的部分数据\n",
    "# dataframe必须有2列: \n",
    "#   - input: 文本输入\n",
    "#   - target: 目标输出\n",
    "df = pd.read_csv('data/prompt/train_prompt.tsv', sep='\\t', encoding='utf8')  # 数据量：1200k数据。\n",
    "# df = df.sample(frac=0.01) # TODO  取消本行代码，如果你需要更多数据训练\n",
    "print(\"df.head:\",df.head(n=5))\n",
    "print(\"df.shape:\",df.shape)\n",
    "# 显存占用说明：如果运行现在显存不足，请使用nvidia-smi查看显存；如果显卡多数被占用了，请重启colab程序\n",
    "T5Trainer(\n",
    "    dataframe=df,\n",
    "    source_text=\"input\",\n",
    "    target_text=\"target\",\n",
    "    model_params=model_params,\n",
    "    output_dir=\"outputs/prompt/\"\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"end..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec9b9e-97ae-4542-a326-a3aee5cccd52",
   "metadata": {},
   "source": [
    "# 预测 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d4253-be9f-4cba-8e32-c59b438ae120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(tokenizer, model, device, loader, max_length):\n",
    "\n",
    "    \"\"\"\n",
    "    用于预测的方法：输入用于预测的数据，返回模型预测的结果\n",
    "    Function for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length=max_length, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            \n",
    "            print(_, preds[0])\n",
    "            if _ % 100 == 0:\n",
    "                console.print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "    return predictions\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1772d9-e5b0-4926-9614-12e9329dfe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练类：整合数据集类、训练方法、验证方法，加载数据进行训练并验证训练过程的效果\n",
    "def T5Tester(\n",
    "    dataframe, source_text, model_params, output_dir=\"./outputs/prompt/\"\n",
    "):\n",
    "    \"\"\"\n",
    "    T5 tester\n",
    "    \"\"\"\n",
    "    # n_gpu = torch.cuda.device_count()\n",
    "    n_gpu = 1\n",
    "    \n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(model_params[\"SEED\"])\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    # Defining the model. We are using PromptCLUE model and added a Language model layer on top for generation of prediction.\n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        # model = model.module.cuda()\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "    # Importing the raw dataset\n",
    "    # dataframe = dataframe[source_text]\n",
    "    # display_df(dataframe.head(2))\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size So 94% of the data will be used for training and the rest for validation.\n",
    "    test_dataset = dataframe\n",
    "    \n",
    "    # 打印数据集相关日志：数据量、训练步数\n",
    "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TEST Dataset: {test_dataset.shape}\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    testing_set = SmallSampleDataSetClass(\n",
    "        test_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text=source_text,\n",
    "        target_text=None\n",
    "    )\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    test_params = {\n",
    "        \"batch_size\": model_params[\"TEST_BATCH_SIZE\"],\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0\n",
    "    }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    testing_loader = DataLoader(testing_set, **test_params)\n",
    "    \n",
    "    # 3) evaluating test dataset\n",
    "    console.log(f\"[Initiating Prediction]...\\n\")\n",
    "    with torch.no_grad(): # add 2022.10.4\n",
    "        #for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "        predictions = testing(tokenizer, model, device, testing_loader, model_params[\"MAX_TARGET_TEXT_LENGTH\"])\n",
    "        final_df = pd.DataFrame({\"Generated Text\": predictions})\n",
    "        final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"), encoding='utf8', index=None)\n",
    "\n",
    "    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
    "\n",
    "    console.log(f\"[Prediction Completed.]\\n\")\n",
    "    console.print(\n",
    "        f\"\"\"[Prediction] Generation on Testing data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n",
    "    )\n",
    "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f1a35-b379-49b7-96aa-36cd3ce2bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型的参数 let's define model parameters specific to T5\n",
    "model_params = {\n",
    "    \"MODEL\": \"outputs/prompt/model_files/\",  # model_type\n",
    "    \"TEST_BATCH_SIZE\": 32,  # training batch size, 8\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 330,  # max length of source text, 512\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 420,  # max length of target text,64\n",
    "    \"SEED\": 2022,  # set seed for reproducibility\n",
    "}\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9dc41-12c5-485f-a31e-135f19eb4b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "# 使用 pCLUE:1200000+多任务提示学习数据集 的部分数据\n",
    "# dataframe必须有2列: \n",
    "#   - input: 文本输入\n",
    "#   - target: 目标输出\n",
    "df = pd.read_csv('data/prompt/testA_prompt.tsv', sep='\\t', encoding='utf8', header=0, names=[\"input\"])  # 数据量：1200k数据。\n",
    "# df = df.sample(frac=0.01) # TODO  取消本行代码，如果你需要更多数据训练\n",
    "print(\"df.head:\",df.head(n=5))\n",
    "print(\"df.shape:\",df.shape)\n",
    "# 显存占用说明：如果运行现在显存不足，请使用nvidia-smi查看显存；如果显卡多数被占用了，请重启colab程序\n",
    "T5Tester(\n",
    "    dataframe=df,\n",
    "    source_text=\"input\",\n",
    "    model_params=model_params,\n",
    "    output_dir=\"outputs/prompt/prediction/\",\n",
    ")\n",
    "torch.cuda.empty_cache() \n",
    "print(\"end..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f370954d-2cf5-4657-87f6-76bb45863340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看训练后显存占用情况。如果显存被占用，可以kill掉相关的进程\n",
    "!nvidia-smi\n",
    "# !fuser -v /dev/nvidia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998828ad-ad79-4f9e-b414-919edc986962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi -r \n",
    "# 使用以下命令清除训练中残存的GPU显存缓存\n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache() \n",
    "torch.cuda.empty_cache()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839dd22-938e-4f30-9623-0f1d020cd270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
