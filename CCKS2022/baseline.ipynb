{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "291f34ea-bcc0-4812-880f-0dd264767911",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 26 00:00:22 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 516.25       Driver Version: 516.25       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000   WDDM  | 00000000:65:00.0 Off |                    0 |\n",
      "| 30%   28C    P8     8W / 230W |      0MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A5000   WDDM  | 00000000:B3:00.0 Off |                    0 |\n",
      "| 30%   27C    P8    13W / 230W |     78MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A      2784    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    1   N/A  N/A      6376    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    1   N/A  N/A     11184    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    1   N/A  N/A     12620    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    1   N/A  N/A     13436    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    1   N/A  N/A     13628    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    1   N/A  N/A     15560    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f250046-64f3-4bd8-89a0-022f154cacb9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CCKS2022基于知识图谱的优质文章识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1faf690-901c-4c59-993f-22e2eefbd138",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!set CUDA_VISIBLE_DEVICES=0,1\n",
    "!python ccks2022_classifier.py \\\n",
    "  --task_name=text-clf \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --data_dir=data/ccks2022/ \\\n",
    "  --bert_model=pretrained_models/nezha-base-wwm \\\n",
    "  --max_seq_length=512 \\\n",
    "  --train_batch_size=28 \\\n",
    "  --eval_batch_size=28 \\\n",
    "  --learning_rate=1e-4  \\\n",
    "  --num_train_epochs=5.0 \\\n",
    "  --output_dir=output/ccks2022/2/\n",
    "  # --warmup_proportion=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccaa96d-7bfd-4794-b74d-55f04f36e1a2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python ccks2022_pred.py \\\n",
    "  --data_dir=data/ccks2022/ \\\n",
    "  --trained_model_dir=trained_models/ccks2022/ \\\n",
    "  --output_dir=output/ccks2022/2/predict/ \\\n",
    "  --eval_batch_size=30 \\\n",
    "  --max_seq_length=512"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 输出结果"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('data/ccks2022/process/test.unlabel.json', encoding='utf8', lines=True)\n",
    "labels = pd.read_csv(\"test_results.txt\", encoding='utf8', header=None, names=['labels'])\n",
    "s = \"\"\n",
    "for i in range(df.shape[0]):\n",
    "    dic = dict()\n",
    "    dic['url'] = df.iloc[i, :][\"url\"]\n",
    "    dic['label'] = int(labels.iloc[i, :][\"labels\"])\n",
    "    s += json.dumps(dic, ensure_ascii=False) + \"\\n\"\n",
    "    if i == 0:\n",
    "        print(s[:100])\n",
    "with open(\"data/ccks2022/process/result/result.txt\", encoding=\"utf-8\", mode='w') as f:\n",
    "    f.write(s)\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 继续预训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init model...\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "Total param num：160336522\n",
      "missing keys:['bert.ptm.embeddings.word_embeddings.weight', 'bert.ptm.embeddings.token_type_embeddings.weight', 'bert.ptm.embeddings.LayerNorm.weight', 'bert.ptm.embeddings.LayerNorm.bias', 'bert.ptm.encoder.layer.0.attention.self.query.weight', 'bert.ptm.encoder.layer.0.attention.self.query.bias', 'bert.ptm.encoder.layer.0.attention.self.key.weight', 'bert.ptm.encoder.layer.0.attention.self.key.bias', 'bert.ptm.encoder.layer.0.attention.self.value.weight', 'bert.ptm.encoder.layer.0.attention.self.value.bias', 'bert.ptm.encoder.layer.0.attention.output.dense.weight', 'bert.ptm.encoder.layer.0.attention.output.dense.bias', 'bert.ptm.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.ptm.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.ptm.encoder.layer.0.intermediate.dense.weight', 'bert.ptm.encoder.layer.0.intermediate.dense.bias', 'bert.ptm.encoder.layer.0.output.dense.weight', 'bert.ptm.encoder.layer.0.output.dense.bias', 'bert.ptm.encoder.layer.0.output.LayerNorm.weight', 'bert.ptm.encoder.layer.0.output.LayerNorm.bias', 'bert.ptm.encoder.layer.1.attention.self.query.weight', 'bert.ptm.encoder.layer.1.attention.self.query.bias', 'bert.ptm.encoder.layer.1.attention.self.key.weight', 'bert.ptm.encoder.layer.1.attention.self.key.bias', 'bert.ptm.encoder.layer.1.attention.self.value.weight', 'bert.ptm.encoder.layer.1.attention.self.value.bias', 'bert.ptm.encoder.layer.1.attention.output.dense.weight', 'bert.ptm.encoder.layer.1.attention.output.dense.bias', 'bert.ptm.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.ptm.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.ptm.encoder.layer.1.intermediate.dense.weight', 'bert.ptm.encoder.layer.1.intermediate.dense.bias', 'bert.ptm.encoder.layer.1.output.dense.weight', 'bert.ptm.encoder.layer.1.output.dense.bias', 'bert.ptm.encoder.layer.1.output.LayerNorm.weight', 'bert.ptm.encoder.layer.1.output.LayerNorm.bias', 'bert.ptm.encoder.layer.2.attention.self.query.weight', 'bert.ptm.encoder.layer.2.attention.self.query.bias', 'bert.ptm.encoder.layer.2.attention.self.key.weight', 'bert.ptm.encoder.layer.2.attention.self.key.bias', 'bert.ptm.encoder.layer.2.attention.self.value.weight', 'bert.ptm.encoder.layer.2.attention.self.value.bias', 'bert.ptm.encoder.layer.2.attention.output.dense.weight', 'bert.ptm.encoder.layer.2.attention.output.dense.bias', 'bert.ptm.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.ptm.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.ptm.encoder.layer.2.intermediate.dense.weight', 'bert.ptm.encoder.layer.2.intermediate.dense.bias', 'bert.ptm.encoder.layer.2.output.dense.weight', 'bert.ptm.encoder.layer.2.output.dense.bias', 'bert.ptm.encoder.layer.2.output.LayerNorm.weight', 'bert.ptm.encoder.layer.2.output.LayerNorm.bias', 'bert.ptm.encoder.layer.3.attention.self.query.weight', 'bert.ptm.encoder.layer.3.attention.self.query.bias', 'bert.ptm.encoder.layer.3.attention.self.key.weight', 'bert.ptm.encoder.layer.3.attention.self.key.bias', 'bert.ptm.encoder.layer.3.attention.self.value.weight', 'bert.ptm.encoder.layer.3.attention.self.value.bias', 'bert.ptm.encoder.layer.3.attention.output.dense.weight', 'bert.ptm.encoder.layer.3.attention.output.dense.bias', 'bert.ptm.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.ptm.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.ptm.encoder.layer.3.intermediate.dense.weight', 'bert.ptm.encoder.layer.3.intermediate.dense.bias', 'bert.ptm.encoder.layer.3.output.dense.weight', 'bert.ptm.encoder.layer.3.output.dense.bias', 'bert.ptm.encoder.layer.3.output.LayerNorm.weight', 'bert.ptm.encoder.layer.3.output.LayerNorm.bias', 'bert.ptm.encoder.layer.4.attention.self.query.weight', 'bert.ptm.encoder.layer.4.attention.self.query.bias', 'bert.ptm.encoder.layer.4.attention.self.key.weight', 'bert.ptm.encoder.layer.4.attention.self.key.bias', 'bert.ptm.encoder.layer.4.attention.self.value.weight', 'bert.ptm.encoder.layer.4.attention.self.value.bias', 'bert.ptm.encoder.layer.4.attention.output.dense.weight', 'bert.ptm.encoder.layer.4.attention.output.dense.bias', 'bert.ptm.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.ptm.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.ptm.encoder.layer.4.intermediate.dense.weight', 'bert.ptm.encoder.layer.4.intermediate.dense.bias', 'bert.ptm.encoder.layer.4.output.dense.weight', 'bert.ptm.encoder.layer.4.output.dense.bias', 'bert.ptm.encoder.layer.4.output.LayerNorm.weight', 'bert.ptm.encoder.layer.4.output.LayerNorm.bias', 'bert.ptm.encoder.layer.5.attention.self.query.weight', 'bert.ptm.encoder.layer.5.attention.self.query.bias', 'bert.ptm.encoder.layer.5.attention.self.key.weight', 'bert.ptm.encoder.layer.5.attention.self.key.bias', 'bert.ptm.encoder.layer.5.attention.self.value.weight', 'bert.ptm.encoder.layer.5.attention.self.value.bias', 'bert.ptm.encoder.layer.5.attention.output.dense.weight', 'bert.ptm.encoder.layer.5.attention.output.dense.bias', 'bert.ptm.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.ptm.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.ptm.encoder.layer.5.intermediate.dense.weight', 'bert.ptm.encoder.layer.5.intermediate.dense.bias', 'bert.ptm.encoder.layer.5.output.dense.weight', 'bert.ptm.encoder.layer.5.output.dense.bias', 'bert.ptm.encoder.layer.5.output.LayerNorm.weight', 'bert.ptm.encoder.layer.5.output.LayerNorm.bias', 'bert.ptm.encoder.layer.6.attention.self.query.weight', 'bert.ptm.encoder.layer.6.attention.self.query.bias', 'bert.ptm.encoder.layer.6.attention.self.key.weight', 'bert.ptm.encoder.layer.6.attention.self.key.bias', 'bert.ptm.encoder.layer.6.attention.self.value.weight', 'bert.ptm.encoder.layer.6.attention.self.value.bias', 'bert.ptm.encoder.layer.6.attention.output.dense.weight', 'bert.ptm.encoder.layer.6.attention.output.dense.bias', 'bert.ptm.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.ptm.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.ptm.encoder.layer.6.intermediate.dense.weight', 'bert.ptm.encoder.layer.6.intermediate.dense.bias', 'bert.ptm.encoder.layer.6.output.dense.weight', 'bert.ptm.encoder.layer.6.output.dense.bias', 'bert.ptm.encoder.layer.6.output.LayerNorm.weight', 'bert.ptm.encoder.layer.6.output.LayerNorm.bias', 'bert.ptm.encoder.layer.7.attention.self.query.weight', 'bert.ptm.encoder.layer.7.attention.self.query.bias', 'bert.ptm.encoder.layer.7.attention.self.key.weight', 'bert.ptm.encoder.layer.7.attention.self.key.bias', 'bert.ptm.encoder.layer.7.attention.self.value.weight', 'bert.ptm.encoder.layer.7.attention.self.value.bias', 'bert.ptm.encoder.layer.7.attention.output.dense.weight', 'bert.ptm.encoder.layer.7.attention.output.dense.bias', 'bert.ptm.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.ptm.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.ptm.encoder.layer.7.intermediate.dense.weight', 'bert.ptm.encoder.layer.7.intermediate.dense.bias', 'bert.ptm.encoder.layer.7.output.dense.weight', 'bert.ptm.encoder.layer.7.output.dense.bias', 'bert.ptm.encoder.layer.7.output.LayerNorm.weight', 'bert.ptm.encoder.layer.7.output.LayerNorm.bias', 'bert.ptm.encoder.layer.8.attention.self.query.weight', 'bert.ptm.encoder.layer.8.attention.self.query.bias', 'bert.ptm.encoder.layer.8.attention.self.key.weight', 'bert.ptm.encoder.layer.8.attention.self.key.bias', 'bert.ptm.encoder.layer.8.attention.self.value.weight', 'bert.ptm.encoder.layer.8.attention.self.value.bias', 'bert.ptm.encoder.layer.8.attention.output.dense.weight', 'bert.ptm.encoder.layer.8.attention.output.dense.bias', 'bert.ptm.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.ptm.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.ptm.encoder.layer.8.intermediate.dense.weight', 'bert.ptm.encoder.layer.8.intermediate.dense.bias', 'bert.ptm.encoder.layer.8.output.dense.weight', 'bert.ptm.encoder.layer.8.output.dense.bias', 'bert.ptm.encoder.layer.8.output.LayerNorm.weight', 'bert.ptm.encoder.layer.8.output.LayerNorm.bias', 'bert.ptm.encoder.layer.9.attention.self.query.weight', 'bert.ptm.encoder.layer.9.attention.self.query.bias', 'bert.ptm.encoder.layer.9.attention.self.key.weight', 'bert.ptm.encoder.layer.9.attention.self.key.bias', 'bert.ptm.encoder.layer.9.attention.self.value.weight', 'bert.ptm.encoder.layer.9.attention.self.value.bias', 'bert.ptm.encoder.layer.9.attention.output.dense.weight', 'bert.ptm.encoder.layer.9.attention.output.dense.bias', 'bert.ptm.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.ptm.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.ptm.encoder.layer.9.intermediate.dense.weight', 'bert.ptm.encoder.layer.9.intermediate.dense.bias', 'bert.ptm.encoder.layer.9.output.dense.weight', 'bert.ptm.encoder.layer.9.output.dense.bias', 'bert.ptm.encoder.layer.9.output.LayerNorm.weight', 'bert.ptm.encoder.layer.9.output.LayerNorm.bias', 'bert.ptm.encoder.layer.10.attention.self.query.weight', 'bert.ptm.encoder.layer.10.attention.self.query.bias', 'bert.ptm.encoder.layer.10.attention.self.key.weight', 'bert.ptm.encoder.layer.10.attention.self.key.bias', 'bert.ptm.encoder.layer.10.attention.self.value.weight', 'bert.ptm.encoder.layer.10.attention.self.value.bias', 'bert.ptm.encoder.layer.10.attention.output.dense.weight', 'bert.ptm.encoder.layer.10.attention.output.dense.bias', 'bert.ptm.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.ptm.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.ptm.encoder.layer.10.intermediate.dense.weight', 'bert.ptm.encoder.layer.10.intermediate.dense.bias', 'bert.ptm.encoder.layer.10.output.dense.weight', 'bert.ptm.encoder.layer.10.output.dense.bias', 'bert.ptm.encoder.layer.10.output.LayerNorm.weight', 'bert.ptm.encoder.layer.10.output.LayerNorm.bias', 'bert.ptm.encoder.layer.11.attention.self.query.weight', 'bert.ptm.encoder.layer.11.attention.self.query.bias', 'bert.ptm.encoder.layer.11.attention.self.key.weight', 'bert.ptm.encoder.layer.11.attention.self.key.bias', 'bert.ptm.encoder.layer.11.attention.self.value.weight', 'bert.ptm.encoder.layer.11.attention.self.value.bias', 'bert.ptm.encoder.layer.11.attention.output.dense.weight', 'bert.ptm.encoder.layer.11.attention.output.dense.bias', 'bert.ptm.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.ptm.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.ptm.encoder.layer.11.intermediate.dense.weight', 'bert.ptm.encoder.layer.11.intermediate.dense.bias', 'bert.ptm.encoder.layer.11.output.dense.weight', 'bert.ptm.encoder.layer.11.output.dense.bias', 'bert.ptm.encoder.layer.11.output.LayerNorm.weight', 'bert.ptm.encoder.layer.11.output.LayerNorm.bias', 'bert.ptm.pooler.dense.weight', 'bert.ptm.pooler.dense.bias', 'bert.gru.weight_ih_l0', 'bert.gru.weight_hh_l0', 'bert.gru.bias_ih_l0', 'bert.gru.bias_hh_l0', 'bert.gru.weight_ih_l0_reverse', 'bert.gru.weight_hh_l0_reverse', 'bert.gru.bias_ih_l0_reverse', 'bert.gru.bias_hh_l0_reverse', 'bert.gru.weight_ih_l1', 'bert.gru.weight_hh_l1', 'bert.gru.bias_ih_l1', 'bert.gru.bias_hh_l1', 'bert.gru.weight_ih_l1_reverse', 'bert.gru.weight_hh_l1_reverse', 'bert.gru.bias_ih_l1_reverse', 'bert.gru.bias_hh_l1_reverse', 'bert.transformer.fc_1.weight', 'bert.transformer.fc_1.bias', 'bert.transformer.fc_2.weight', 'bert.transformer.fc_2.bias', 'bert.transformer.fc_3.weight', 'bert.transformer.fc_3.bias', 'bert.transformer.fc_4.weight', 'bert.transformer.fc_4.bias', 'bert.transformer.fc_5.weight', 'bert.transformer.fc_5.bias', 'bert.transformer.fc_6.weight', 'bert.transformer.fc_6.bias', 'bert.transformer.q.weight', 'bert.transformer.q.bias', 'bert.transformer.k.weight', 'bert.transformer.k.bias', 'bert.transformer.v.weight', 'bert.transformer.v.bias', 'bert.transformer.layer_norm_2.weight', 'bert.transformer.layer_norm_2.bias', 'bert.transformer.layer_norm.weight', 'bert.transformer.layer_norm.bias', 'bert.linear.weight', 'bert.linear.bias']\n",
      "unexpected keys:['bert.embeddings.word_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "error msgs:[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "\n",
      "Iteration:   0%|          | 0/916 [00:00<?, ?it/s]\u001B[AC:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:975: block: [193,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "\n",
      "Iteration:   0%|          | 0/916 [00:00<?, ?it/s]\n",
      "\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Projects\\CCKS2022\\ccks2022_for_pretraining.py\", line 773, in <module>\n",
      "    main()\n",
      "  File \"D:\\Projects\\CCKS2022\\ccks2022_for_pretraining.py\", line 579, in main\n",
      "    loss = model(input_ids=masked_input_ids, token_type_ids=segment_ids, attention_mask=input_mask,\n",
      "  File \"C:\\Anaconda3\\envs\\pytorch39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"D:\\Projects\\CCKS2022\\modeling.py\", line 22, in forward\n",
      "    encoder_out, _ = self.ptm(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
      "  File \"C:\\Anaconda3\\envs\\pytorch39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"D:\\Projects\\CCKS2022\\modeling_nezha_ccks2022.py\", line 710, in forward\n",
      "    embedding_output = self.embeddings(input_ids, token_type_ids)\n",
      "  File \"C:\\Anaconda3\\envs\\pytorch39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"D:\\Projects\\CCKS2022\\modeling_nezha_ccks2022.py\", line 232, in forward\n",
      "    embeddings = self.LayerNorm(embeddings)\n",
      "  File \"C:\\Anaconda3\\envs\\pytorch39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"D:\\Projects\\CCKS2022\\modeling_nezha_ccks2022.py\", line 193, in forward\n",
      "    s = (x - u).pow(2).mean(-1, keepdim=True)\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n"
     ]
    }
   ],
   "source": [
    "!set CUDA_VISIBLE_DEVICES=0,1\n",
    "!python ccks2022_for_pretraining.py \\\n",
    "  --task_name=text-clf \\\n",
    "  --do_train \\\n",
    "  --data_dir=data/ccks2022/ \\\n",
    "  --bert_model=pretrained_models/nezha-cn-base \\\n",
    "  --max_seq_length=512 \\\n",
    "  --train_batch_size=8 \\\n",
    "  --eval_batch_size=8 \\\n",
    "  --learning_rate=3e-5  \\\n",
    "  --num_train_epochs=5.0 \\\n",
    "  --output_dir=output/ccks2022/continue_pretraining/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}