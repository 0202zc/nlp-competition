{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#安装相关依赖库 如果是windows系统，cmd命令框中输入pip安装，参考上述环境配置\n",
    "#!pip install sklearn\n",
    "#!pip install pandas\n",
    "#---------------------------------------------------\n",
    "#导入库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "train_df = pd.read_json('./data/train.json', encoding='utf8', lines=True)\n",
    "testA_df = pd.read_json('./data/testA.json', encoding='utf8', lines=True)\n",
    "\n",
    "#----------------特征工程----------------\n",
    "#将论文的标题与摘要组合为 text 特征\n",
    "train_df['title'] = train_df['title'].apply(lambda x: x.strip())\n",
    "train_df['abstract'] = train_df['abstract'].fillna('').apply(lambda x: x.strip())\n",
    "train_df['text'] = train_df['title'].str.lower() + '[SEP]' + train_df['abstract'].str.lower()\n",
    "\n",
    "testA_df['title'] = testA_df['title'].apply(lambda x: x.strip())\n",
    "testA_df['abstract'] = testA_df['abstract'].fillna('').apply(lambda x: x.strip())\n",
    "testA_df['text'] = testA_df['title'].str.lower() + '[SEP]' + testA_df['abstract'].str.lower()\n",
    "\n",
    "#使用tfidf算法做文本特征提取\n",
    "tfidf = TfidfVectorizer(max_features=2500)\n",
    "\n",
    "#----------------模型训练----------------\n",
    "\n",
    "train_tfidf = tfidf.fit_transform(train_df['text'])\n",
    "clf = SGDClassifier()\n",
    "cross_val_score(clf, train_tfidf, train_df['label_id'], cv=5)\n",
    "\n",
    "test_tfidf = tfidf.transform(testA_df['text'])\n",
    "clf = SGDClassifier()\n",
    "clf.fit(train_tfidf, train_df['label_id'])\n",
    "testA_df['label'] = clf.predict(test_tfidf)\n",
    "\n",
    "#----------------结果输出----------------\n",
    "testA_df[['id', 'label']].to_csv('submit/submit (tf-idf).csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 训练集中词频统计，并计算TF值\n",
    "def words_tf():\n",
    "    train_data = pd.read_csv('data/train.tsv', sep='\\t').astype(str)\n",
    "    sentence_list = []\n",
    "    lenth = len(train_data)\n",
    "    for i in range(lenth):\n",
    "        sentence_list.append(str(train_data['text'][i]).split())\n",
    "    # 总词频统计\n",
    "    doc_frequency = defaultdict(int)\n",
    "    for word_list in sentence_list:\n",
    "        for i in word_list:\n",
    "            doc_frequency[i] += 1\n",
    "\n",
    "    # 计算每个词的TF值\n",
    "    word_tf = {}  # 存储每个词的tf值\n",
    "    for i in doc_frequency:\n",
    "        word_tf[i] = doc_frequency[i] / sum(doc_frequency.values())\n",
    "\n",
    "    words_tf = sorted(word_tf.items(), key=lambda x: x[1], reverse=True)\n",
    "    return words_tf[:10000]\n",
    "\n",
    "\n",
    "# 根据词频，将文本转换为向量\n",
    "def word2vec(keywords_tf, doc_sentence):\n",
    "    keywords = list(dict(keywords_tf).keys())  # 获取关键词\n",
    "    tf_weight = list(dict(keywords_tf).values())  # 获取关键词tf值\n",
    "\n",
    "    docvec_list = []\n",
    "    for sentence in doc_sentence:\n",
    "        docvec = [0] * len(keywords_tf)\n",
    "        for word in sentence:\n",
    "            if word in keywords:\n",
    "                docvec[keywords.index(word)] = tf_weight[keywords.index(word)]\n",
    "        docvec_list.append(docvec)\n",
    "    return docvec_list\n",
    "\n",
    "\n",
    "# 将训练集和测试集换为文本向量\n",
    "def doc_vec(x_train, x_test):\n",
    "    keywords_tf = words_tf()  # 获取词频关键词\n",
    "\n",
    "    # 训练集转换为向量\n",
    "    train_lenth = len(x_train)\n",
    "    train_data_list = []\n",
    "    for i in range(train_lenth):\n",
    "        train_data_list.append(str(x_train[i]).split())\n",
    "    train_docvec_list = word2vec(keywords_tf, train_data_list)\n",
    "\n",
    "    # 测试集转换为向量\n",
    "    test_lenth = len(x_test)\n",
    "    test_data_list = []\n",
    "    for i in range(test_lenth):\n",
    "        test_data_list.append(str(x_test[i]).split())\n",
    "    test_docvec_list = word2vec(keywords_tf, test_data_list)\n",
    "\n",
    "    return train_docvec_list, test_docvec_list\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_data = pd.read_csv('data/train.tsv', sep='\\t').astype(str)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_data['text'], train_data['label_id'], test_size=0.05)\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    x_test = np.array(x_test)\n",
    "    cw = lambda x: int(x)\n",
    "    y_train = np.array(y_train.apply(cw))\n",
    "    y_test = np.array(y_test.apply(cw))\n",
    "\n",
    "    x_train, x_test = doc_vec(x_train, x_test)  # 训练集和测试集向量化\n",
    "    x_train, y_train = shuffle(x_train, y_train, random_state=0)  # 打乱顺序\n",
    "\n",
    "    # 导入SelectFromModel结合ExtraTreesClassifier计算特征重要性，并按重要性阈值选择特征。\n",
    "    clf_model = ExtraTreesClassifier(n_estimators=250, random_state=0)\n",
    "    # clf_model=RandomForestClassifier(n_estimators=250,random_state=0)\n",
    "    clf_model.fit(x_train, y_train)\n",
    "    # 获取每个词的特征权重,数值越高特征越重要l\n",
    "    importances = clf_model.feature_importances_\n",
    "\n",
    "    '''\n",
    "    # 将词和词的权重存入字典并写入文件\n",
    "    feature_words_dic = {}\n",
    "    for i in range(len(words_list)):\n",
    "        feature_words_dic[words_list[i][0]] = importances[i]\n",
    "    # 对字典按权重由大到小进行排序\n",
    "    words_info_dic_sort = sorted(feature_words_dic.items(), key=lambda x: x[1], reverse=True)\n",
    "    #将前2000个词的权重字典写入文件\n",
    "    key_words_importance=dict(words_info_dic_sort[:2000])\n",
    "    with open('data/key_words_importance','w') as f:\n",
    "        f.write(str(key_words_importance))\n",
    "    '''\n",
    "\n",
    "    # 选择特征重要性为1.5倍均值的特征\n",
    "    model = SelectFromModel(clf_model, threshold='1.5*mean', prefit=True)\n",
    "    x_train_new = model.transform(x_train)  # 返回训练集所选特征\n",
    "    x_test_new = model.transform(x_test)  # 返回测试集所选特征\n",
    "\n",
    "    print(x_train_new.shape)\n",
    "    print(x_test_new.shape)\n",
    "\n",
    "    # 创建成lgb特征的数据集格式\n",
    "    lgb_train = lgb.Dataset(x_train_new, y_train)\n",
    "    lgb_val = lgb.Dataset(x_test_new, y_test, reference=lgb_train)\n",
    "\n",
    "    # 构建lightGBM模型\n",
    "    params = {'max_depth': 6, 'min_data_in_leaf': 20, 'num_leaves': 35, 'learning_rate': 0.1, 'lambda_l1': 0.1,\n",
    "              'lambda_l2': 0.2, 'objective': 'multiclass', 'num_class': 36, 'verbose': -1}\n",
    "    # 设置迭代次数，默认为100，通常设置为100+\n",
    "    num_boost_round = 2000\n",
    "    # 训练lightGBM模型\n",
    "    gbm = lgb.train(params, lgb_train, num_boost_round, verbose_eval=100, valid_sets=lgb_val)\n",
    "\n",
    "    # 保存模型到文件\n",
    "    # gbm.save_model('data/lightGBM_model')\n",
    "\n",
    "    # 预测数据集\n",
    "    result = gbm.predict(x_test_new, num_iteration=gbm.best_iteration)\n",
    "    y_predict = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
    "\n",
    "    label_all = [i for i in range(25)]\n",
    "    confusion_mat = metrics.confusion_matrix(y_test, y_predict)\n",
    "    df = pd.DataFrame(confusion_mat, columns=label_all)\n",
    "    df.index = label_all\n",
    "\n",
    "    print('准确率：', metrics.accuracy_score(y_test, y_predict))\n",
    "    print('confusion_matrix:', df)\n",
    "    print('分类报告:', metrics.classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 折投票融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "output_file = 'submit/submit (5fold-base-attention-fgm-labeled-p_tuning16-bert).csv'\n",
    "\n",
    "df0 = pd.read_csv('results/20/test_results_0.txt', header=None, names=['label'], encoding='utf8')\n",
    "df1 = pd.read_csv('results/20/test_results_1.txt', header=None, names=['label'], encoding='utf8')\n",
    "df2 = pd.read_csv('results/20/test_results_2.txt', header=None, names=['label'], encoding='utf8')\n",
    "df3 = pd.read_csv('results/20/test_results_3.txt', header=None, names=['label'], encoding='utf8')\n",
    "df4 = pd.read_csv('results/20/test_results_4.txt', header=None, names=['label'], encoding='utf8')\n",
    "df5 = pd.read_csv('submit/submit (voting fusion 0.5885).csv', encoding='utf8')\n",
    "df6 = pd.read_csv('submit/submit (voting fusion 0.5940).csv', encoding='utf8')\n",
    "# df7 = pd.read_csv('results/18/test_results_5.txt', header=None, names=['label'], encoding='utf8')\n",
    "# df8 = pd.read_csv('results/18/test_results_6.txt', header=None, names=['label'], encoding='utf8')\n",
    "# df5 = pd.read_csv('results/8/test_results_0.txt', header=None, names=['label'], encoding='utf8')\n",
    "# df6 = pd.read_csv('results/8/test_results_1.txt', header=None, names=['label'], encoding='utf8')\n",
    "# df7 = pd.read_csv('results/8/test_results_2.txt', header=None, names=['label'], encoding='utf8')\n",
    "# df8 = pd.read_csv('results/8/test_results_3.txt', header=None, names=['label'], encoding='utf8')\n",
    "# df9 = pd.read_csv('results/8/test_results_4.txt', header=None, names=['label'], encoding='utf8')\n",
    "# df10 = pd.read_csv('results/12/test_results_0.txt', header=None, names=['label'], encoding='utf8')\n",
    "# df11 = pd.read_csv('results/12/test_results_1.txt', header=None, names=['label'], encoding='utf8')\n",
    "# df12 = pd.read_csv('results/12/test_results_2.txt', header=None, names=['label'], encoding='utf8')\n",
    "# df13 = pd.read_csv('results/12/test_results_3.txt', header=None, names=['label'], encoding='utf8')\n",
    "# df14 = pd.read_csv('results/14/test_results_4.txt', header=None, names=['label'], encoding='utf8')\n",
    "testA_df = pd.read_json('data/testA.json', encoding='utf8', lines=True)\n",
    "\n",
    "count = np.zeros((df0.shape[0], 36), np.int64)\n",
    "df_out = pd.DataFrame(data=df0)\n",
    "for i in range(df0.shape[0]):\n",
    "    count[i][df0['label'].iloc[i]] += 1\n",
    "    count[i][df1['label'].iloc[i]] += 1\n",
    "    count[i][df2['label'].iloc[i]] += 1\n",
    "    count[i][df3['label'].iloc[i]] += 1\n",
    "    count[i][df4['label'].iloc[i]] += 1\n",
    "    # count[i][df5['label'].iloc[i]] += 1\n",
    "    # count[i][df6['label'].iloc[i]] += 1\n",
    "    # count[i][df7['label'].iloc[i]] += 1\n",
    "    # count[i][df8['label'].iloc[i]] += 1\n",
    "    # count[i][df9['label'].iloc[i]] += 1\n",
    "    df_out['label'].iloc[i] = np.argmax(count[i])\n",
    "\n",
    "testA_df['label'] = df_out['label']\n",
    "testA_df[['id', 'label']].to_csv(output_file, index=None)\n",
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "output_file = 'submit/submit (5fold-base-attention-fgm-labeled-3).csv'\n",
    "\n",
    "df = pd.read_csv('results/test_results_3 (base + attention + fgm + labeled).txt', header=None, names=['label'],\n",
    "                 encoding='utf8')\n",
    "testA_df = pd.read_json('data/testA.json', encoding='utf8', lines=True)\n",
    "testA_df['label'] = df['label']\n",
    "testA_df[['id', 'label']].to_csv(output_file, index=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果相关性计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[                                     id  label  \\\n 892    71122c339f20cd2c76dd573771979af4      8   \n 1195   0fb259d65d569e651191819092292c80      2   \n 1493   c16733f8f6e3f8a36cc291740b20d9e1     20   \n 1505   081fa1f604f8adf4834af91010b985d5      0   \n 2283   ad2419ad19d77ff038b7d9df79df8e49      2   \n ...                                 ...    ...   \n 20809  3d1b889207881657a7f0e12b6cf447a5      5   \n 20815  ade8ab925ebe784ac9350ad3159c8cd5      6   \n 20823  051b0af0b9cb38b8bcea5643035ad9ae     11   \n 20826  08dd1ad5a98376d5cd3354e5f6237d52     13   \n 20836  c0fd3051ce51166e80b9922c97e2f7a4      9   \n \n                                      id  label  \n 892    71122c339f20cd2c76dd573771979af4     10  \n 1195   0fb259d65d569e651191819092292c80     17  \n 1493   c16733f8f6e3f8a36cc291740b20d9e1     30  \n 1505   081fa1f604f8adf4834af91010b985d5      4  \n 2283   ad2419ad19d77ff038b7d9df79df8e49     12  \n ...                                 ...    ...  \n 20809  3d1b889207881657a7f0e12b6cf447a5      7  \n 20815  ade8ab925ebe784ac9350ad3159c8cd5      9  \n 20823  051b0af0b9cb38b8bcea5643035ad9ae      0  \n 20826  08dd1ad5a98376d5cd3354e5f6237d52     18  \n 20836  c0fd3051ce51166e80b9922c97e2f7a4     10  \n \n [825 rows x 4 columns],\n 0.9604107682710303,\n 20014,\n 20839,\n 825]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def correlation_calculating(file1: str, file2: str) -> list:\n",
    "    result = []\n",
    "    if file1.__contains__('txt'):\n",
    "        result1 = pd.read_csv(file1, header=None, names=['label'], encoding='utf8')\n",
    "    else:\n",
    "        result1 = pd.read_csv(file1, encoding='utf8')\n",
    "    if file2.__contains__('txt'):\n",
    "        result2 = pd.read_csv(file2, header=None, names=['label'], encoding='utf8')\n",
    "    else:\n",
    "        result2 = pd.read_csv(file2, encoding='utf8')\n",
    "    same, sum = result1[result1['label'] == result2['label']].shape[0], result1.shape[0]\n",
    "    # result = result2[result1['label'] != result2['label']]\n",
    "    result = pd.concat((result1[result1['label'] != result2['label']], result2[result1['label'] != result2['label']]), axis=1)\n",
    "    correlation = same / sum\n",
    "\n",
    "    return [result, correlation, same, sum, sum - same]\n",
    "\n",
    "\n",
    "# correlation_calculating(\"results/35/test_results_3.txt\", \"results/36/test_results_0.txt\"), correlation_calculating(\n",
    "#     \"results/35/test_results_1.txt\", \"results/36/test_results_0.txt\"), correlation_calculating(\n",
    "#     \"results/35/test_results_3.txt\", \"results/36/test_results_1.txt\"), correlation_calculating(\n",
    "#     \"results/35/test_results_1.txt\", \"results/36/test_results_1.txt\"), correlation_calculating(\n",
    "#     \"results/35/test_results_3.txt\", \"results/36/test_results_2.txt\"), correlation_calculating(\n",
    "#     \"results/35/test_results_1.txt\", \"results/36/test_results_2.txt\"), correlation_calculating(\n",
    "#     \"results/35/test_results_3.txt\", \"results/36/test_results_3.txt\"), correlation_calculating(\n",
    "#     \"results/35/test_results_1.txt\", \"results/36/test_results_3.txt\")\n",
    "\n",
    "# correlation_calculating(\"results/43/epoch3/test_results_0.txt\",\n",
    "#                         \"results/43/epoch3/test_results_2.txt\")\n",
    "correlation_calculating(\"results/62/5/submit (6452_bactrans_patents28000high_4epochs_rdrop result_0 6453).csv\",\n",
    "                        \"results/63/4/submit (6453_bactrans_sampling10_4epochs_rdrop result_0 6325).csv\")\n",
    "\n",
    "# correlation_calculating(\"submit/633/submit (voting fusion).csv\",\n",
    "#                         \"submit/633/submit (6336oversampling_epoch4 result_4 6337).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[                                     id  label  label\n 16     b76077d7d31ca63e38f44d64fa8bcff4      8     21\n 64     e8a7cd421a3f82ca2414fca8fc156149      8     20\n 66     364418d082c0b7ff0cc0e55ff383fc1c      2     22\n 82     c7d4604abd352950b1e1c4b73f0c6641      8     20\n 121    4edfc37517d0875864abc42ca36a8ac6      2     10\n ...                                 ...    ...    ...\n 20796  658e0990cc1eb806ab44c6029ef60635     27      2\n 20800  a1eb4752a7198c007b7036ad7398e29d     11     23\n 20802  fc282214596e62fe0a7c0a81c9be7743     20     25\n 20823  051b0af0b9cb38b8bcea5643035ad9ae      0     11\n 20836  c0fd3051ce51166e80b9922c97e2f7a4     10      9\n \n [802 rows x 3 columns],\n 0.9615144680646864,\n 20037,\n 20839,\n 802]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_calculating(\"results/63/4/submit (6453_bactrans_sampling10_4epochs_rdrop result_0 6325).csv\",\n",
    "                        \"results/63/6/test_results_0.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def correlation_calculating(file1: str, file2: str) -> list:\n",
    "    result1 = pd.read_csv(file1, header=None, names=['label'], encoding='utf8')\n",
    "    result2 = pd.read_csv(file2, header=None, names=['label'], encoding='utf8')\n",
    "    same, sum = result1[result1['label'] == result2['label']].shape[0], result1.shape[0]\n",
    "    correlation = same / sum\n",
    "\n",
    "    return [correlation, same, sum]\n",
    "\n",
    "\n",
    "def get_files(file_path='./submit') -> list:\n",
    "    file_list = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(file_path):\n",
    "        for filename in filenames:\n",
    "            file = os.path.join(dirpath, filename).replace('\\\\', '/')\n",
    "            if filename.__contains__('.txt') or filename.__contains__('.csv1'):\n",
    "                file_list.append(file)\n",
    "    return file_list\n",
    "\n",
    "\n",
    "def get_correlations(file_path='./submit', threshold=None, exclude_files=None) -> list:\n",
    "    file_list = get_files(file_path)\n",
    "    correlations = []\n",
    "\n",
    "    if exclude_files is not None:\n",
    "        for ef in exclude_files:\n",
    "            if file_list.__contains__(ef):\n",
    "                file_list.remove(ef)\n",
    "\n",
    "    length = len(file_list)\n",
    "\n",
    "    for i in range(length - 1):\n",
    "        for j in range(i + 1, length):\n",
    "            file1, file2 = file_list[i], file_list[j]\n",
    "            correlation = correlation_calculating(file1, file2)[0]\n",
    "            if threshold is None or correlation >= threshold:\n",
    "                correlations.append((correlation, file1[-18:], file2[-18:]))\n",
    "                # print(correlation, file1[16:-4], file2[16:-4], sep=\"\\t\")\n",
    "\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def get_elem(elem):\n",
    "    return elem[0]\n",
    "\n",
    "\n",
    "res = get_correlations(file_path='./results/61/', exclude_files=['./submit/submit (tf-idf).csv'])\n",
    "res.sort(reverse=True, key=get_elem)\n",
    "pd.DataFrame(data=res, columns=['correlation', 'file1', 'file2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提交结果投票融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def get_files(file_path='./submit') -> list:\n",
    "    file_list = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(file_path):\n",
    "        for filename in filenames:\n",
    "            file = os.path.join(dirpath, filename).replace('\\\\', '/')\n",
    "            if filename.__contains__('.txt') or filename.__contains__('.csv'):\n",
    "                file_list.append(file)\n",
    "    return file_list\n",
    "\n",
    "\n",
    "def voting_fusion(files: list, output_file='./submit/submit (voting fusion).csv', exclude_files=None):\n",
    "    if files is None or len(files) == 0:\n",
    "        return None\n",
    "    if exclude_files is not None and type(exclude_files) is list:\n",
    "        for ef in exclude_files:\n",
    "            if files.__contains__(ef):\n",
    "                files.remove(ef)\n",
    "    if files.__contains__(output_file):\n",
    "        files.remove(output_file)\n",
    "\n",
    "    print(files)\n",
    "\n",
    "    df0 = pd.read_csv(files[0], encoding='utf8', header=None, names=['label']) if files[0].__contains__(\n",
    "        'txt') else pd.read_csv(files[0], encoding='utf8')\n",
    "    testA_df = pd.read_json('data/testA.json', encoding='utf8', lines=True)\n",
    "    count = np.zeros((df0.shape[0], 36), np.int64)\n",
    "    df_out = pd.DataFrame(data=df0)\n",
    "\n",
    "    for f in files:\n",
    "        print('processing:', f)\n",
    "        df = pd.read_csv(f, encoding='utf8', header=None, names=['label']) if f.__contains__('txt') else pd.read_csv(f,\n",
    "                                                                                                                     encoding='utf8')\n",
    "        for i in range(df.shape[0]):\n",
    "            count[i][df['label'].iloc[i]] += 1\n",
    "            df_out['label'].iloc[i] = np.argmax(count[i])\n",
    "\n",
    "    count_list = np.asmatrix(count).tolist()\n",
    "    with open('voting_matrix.txt', mode='w', encoding='utf8') as f:\n",
    "        for c in count_list:\n",
    "            f.write(str(c) + '\\n')\n",
    "\n",
    "    testA_df['label'] = df_out['label']\n",
    "    testA_df[['id', 'label']].to_csv(output_file, index=None)\n",
    "    print('融合结果已保存：', output_file)\n",
    "    return output_file, count_list\n",
    "\n",
    "\n",
    "file_list = get_files(file_path='./submit/64/')\n",
    "voting_fusion(file_list, output_file='./submit/64/submit (voting fusion).csv',\n",
    "              exclude_files=['../results/28/submit (6122 result_2 6122).csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('submit/submit (5fold + nezha-large-wwm + attention + fgm).csv', header=None, names=['label'],\n",
    "                  encoding='utf8')\n",
    "df2 = pd.read_csv('submit/submit (5fold-attention).csv', header=None, names=['label'], encoding='utf8')\n",
    "df3 = pd.read_csv('submit/submit (5fold-attention).csv', header=None, names=['label'], encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高置信度样本（伪标签）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 使用 5fold + nezha-large-wwm + attention + fgm 方法 且 预测概率在85%以上\n",
    "ids = np.asarray(\n",
    "    pd.read_csv(\"results/59/6/high_confidence_ids.txt\", encoding='utf8', header=None, names=['high_confidence_id'])[\n",
    "        'high_confidence_id']).transpose().tolist()\n",
    "label = pd.read_csv(\"results/59/6/test_results.txt\", encoding='utf8', header=None, names=['label'])\n",
    "# testA_df = pd.read_json('./data/testA.json', encoding='utf8', lines=True)\n",
    "testA_df['label'] = label['label']\n",
    "\n",
    "with open('data/testA_labeled_6440.tsv', mode='w', encoding='utf8') as f:\n",
    "    for i in ids:\n",
    "        id = testA_df.loc[[i]].id.tolist()[0]\n",
    "        title = testA_df.loc[[i]].title.tolist()[0]\n",
    "        assignee = testA_df.loc[[i]].assignee.tolist()[0]\n",
    "        abstract = testA_df.loc[[i]].abstract.tolist()[0]\n",
    "        text = str(testA_df.loc[[i]].text.tolist()[0]).replace('<i>', ' ').replace('</i>', ' ')\n",
    "        label = testA_df.loc[[i]].label.tolist()[0]\n",
    "        f.write(str(label) + '\\t' + text + '\\n')\n",
    "\n",
    "testA_df.loc[[ids[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                     id                      title  \\\n0      003fd481e65ddc070e38ae05002e16e2       一种耐磨、抗粘钢复合涂层、制备方法及应用   \n1      549a1cd8228bd10f18395a0625fcc70d   一种用于提高橡胶抗湿滑性的树脂的制备方法及其应用   \n2      f09c4c0332f8966400e06f4def9f1a6d    有机硅改性丙烯酸树脂超亲水防雾涂料及其制作方法   \n3      06598dd8f3ab092acf2a55dce8be5621          一种空调系统及其控制方法、控制装置   \n4      e70177ba6a54d08abecd80a60fdd9f52        资源申请、分配方法，UE及网络控制单元   \n...                                 ...                        ...   \n20834  befab80c8c6cf6f8db5a4ee3b9e22020       由低合金碳钢制成的螺钉和制造该螺钉的方法   \n20835  b41abe927240b1ab73b1cb0fca2d9970              一种铸造铝合金及其制备方法   \n20836  c0fd3051ce51166e80b9922c97e2f7a4         一种显示面板及生成随机图块坐标的方法   \n20837  4c89a2b2bd405456e316a35411297b0f  一种确定机器类通信下行控制信道重复次数的方法及基站   \n20838  07870847de45f9e8fd081bd2edc7c252    一类层状钙钛矿型纳米氧化物的超级电容器电极材料   \n\n             assignee                                           abstract  \\\n0      安徽马钢表面技术股份有限公司  本发明公开了一种耐磨、抗粘钢复合涂层、制备方法及应用，包括基体和基体上由内到外依次设置的过渡...   \n1        江苏麒祥高新材料有限公司  本发明公开了一种用于提高橡胶抗湿滑性的树脂的制备方法，第一步：将R树脂和B官能团化合物进行反...   \n2                重庆大学  本发明涉及涂料制造领域，本发明公开了一种含有磺酸季铵盐的有机硅改性丙烯酸树脂超亲水低温防雾涂...   \n3          海尔智家股份有限公司  本发明涉及空调领域，公开了一种空调系统，包括室外机和太阳能供热系统，所述太阳能供热系统包括：...   \n4          中兴通讯股份有限公司  本发明实施例公开了一种资源申请方法及装置，所述方法包括：向网络控制单元发送低时延业务信息；接...   \n...               ...                                                ...   \n20834       伊卓特有限两合公司  本发明涉及一种螺钉，该螺钉具有头部、邻接的保持段和功能端。所述螺钉用于自攻螺钉。所述功能端的...   \n20835    中国兵器工业第五九研究所  本发明提供了一种铸造铝合金及其制备方法，铸造铝合金成分包括：Si：7.5～8.5%、Cu：2...   \n20836     上海天马微电子有限公司  本发明提供一种显示面板，显示面板的图块具有随机排布的特性，能够消除由于周期性排布带来的鬼影问...   \n20837       电信科学技术研究院  本发明实施例涉及无线通信技术领域，特别涉及一种确定机器类通信下行控制信道重复次数的方法及基站...   \n20838            云南大学  本发明涉及一类层状钙钛矿型纳米氧化物的超级电容器电极材料，它具有化学式为LXMO的214型层...   \n\n       label                                               text  \n0         23  一种耐磨、抗粘钢复合涂层、制备方法及应用<sep>本发明公开了一种耐磨、抗粘钢复合涂层、制备...  \n1          5  一种用于提高橡胶抗湿滑性的树脂的制备方法及其应用<sep>本发明公开了一种用于提高橡胶抗湿滑...  \n2          5  有机硅改性丙烯酸树脂超亲水防雾涂料及其制作方法<sep>本发明涉及涂料制造领域，本发明公开了...  \n3         16  一种空调系统及其控制方法、控制装置<sep>本发明涉及空调领域，公开了一种空调系统，包括室外...  \n4          0  资源申请、分配方法，ue及网络控制单元<sep>本发明实施例公开了一种资源申请方法及装置，所...  \n...      ...                                                ...  \n20834     23  由低合金碳钢制成的螺钉和制造该螺钉的方法<sep>本发明涉及一种螺钉，该螺钉具有头部、邻接的...  \n20835     13  一种铸造铝合金及其制备方法<sep>本发明提供了一种铸造铝合金及其制备方法，铸造铝合金成分包...  \n20836      9  一种显示面板及生成随机图块坐标的方法<sep>本发明提供一种显示面板，显示面板的图块具有随机...  \n20837      0  一种确定机器类通信下行控制信道重复次数的方法及基站<sep>本发明实施例涉及无线通信技术领域...  \n20838     21  一类层状钙钛矿型纳米氧化物的超级电容器电极材料<sep>本发明涉及一类层状钙钛矿型纳米氧化物...  \n\n[20839 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>assignee</th>\n      <th>abstract</th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>003fd481e65ddc070e38ae05002e16e2</td>\n      <td>一种耐磨、抗粘钢复合涂层、制备方法及应用</td>\n      <td>安徽马钢表面技术股份有限公司</td>\n      <td>本发明公开了一种耐磨、抗粘钢复合涂层、制备方法及应用，包括基体和基体上由内到外依次设置的过渡...</td>\n      <td>23</td>\n      <td>一种耐磨、抗粘钢复合涂层、制备方法及应用&lt;sep&gt;本发明公开了一种耐磨、抗粘钢复合涂层、制备...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>549a1cd8228bd10f18395a0625fcc70d</td>\n      <td>一种用于提高橡胶抗湿滑性的树脂的制备方法及其应用</td>\n      <td>江苏麒祥高新材料有限公司</td>\n      <td>本发明公开了一种用于提高橡胶抗湿滑性的树脂的制备方法，第一步：将R树脂和B官能团化合物进行反...</td>\n      <td>5</td>\n      <td>一种用于提高橡胶抗湿滑性的树脂的制备方法及其应用&lt;sep&gt;本发明公开了一种用于提高橡胶抗湿滑...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>f09c4c0332f8966400e06f4def9f1a6d</td>\n      <td>有机硅改性丙烯酸树脂超亲水防雾涂料及其制作方法</td>\n      <td>重庆大学</td>\n      <td>本发明涉及涂料制造领域，本发明公开了一种含有磺酸季铵盐的有机硅改性丙烯酸树脂超亲水低温防雾涂...</td>\n      <td>5</td>\n      <td>有机硅改性丙烯酸树脂超亲水防雾涂料及其制作方法&lt;sep&gt;本发明涉及涂料制造领域，本发明公开了...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>06598dd8f3ab092acf2a55dce8be5621</td>\n      <td>一种空调系统及其控制方法、控制装置</td>\n      <td>海尔智家股份有限公司</td>\n      <td>本发明涉及空调领域，公开了一种空调系统，包括室外机和太阳能供热系统，所述太阳能供热系统包括：...</td>\n      <td>16</td>\n      <td>一种空调系统及其控制方法、控制装置&lt;sep&gt;本发明涉及空调领域，公开了一种空调系统，包括室外...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>e70177ba6a54d08abecd80a60fdd9f52</td>\n      <td>资源申请、分配方法，UE及网络控制单元</td>\n      <td>中兴通讯股份有限公司</td>\n      <td>本发明实施例公开了一种资源申请方法及装置，所述方法包括：向网络控制单元发送低时延业务信息；接...</td>\n      <td>0</td>\n      <td>资源申请、分配方法，ue及网络控制单元&lt;sep&gt;本发明实施例公开了一种资源申请方法及装置，所...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20834</th>\n      <td>befab80c8c6cf6f8db5a4ee3b9e22020</td>\n      <td>由低合金碳钢制成的螺钉和制造该螺钉的方法</td>\n      <td>伊卓特有限两合公司</td>\n      <td>本发明涉及一种螺钉，该螺钉具有头部、邻接的保持段和功能端。所述螺钉用于自攻螺钉。所述功能端的...</td>\n      <td>23</td>\n      <td>由低合金碳钢制成的螺钉和制造该螺钉的方法&lt;sep&gt;本发明涉及一种螺钉，该螺钉具有头部、邻接的...</td>\n    </tr>\n    <tr>\n      <th>20835</th>\n      <td>b41abe927240b1ab73b1cb0fca2d9970</td>\n      <td>一种铸造铝合金及其制备方法</td>\n      <td>中国兵器工业第五九研究所</td>\n      <td>本发明提供了一种铸造铝合金及其制备方法，铸造铝合金成分包括：Si：7.5～8.5%、Cu：2...</td>\n      <td>13</td>\n      <td>一种铸造铝合金及其制备方法&lt;sep&gt;本发明提供了一种铸造铝合金及其制备方法，铸造铝合金成分包...</td>\n    </tr>\n    <tr>\n      <th>20836</th>\n      <td>c0fd3051ce51166e80b9922c97e2f7a4</td>\n      <td>一种显示面板及生成随机图块坐标的方法</td>\n      <td>上海天马微电子有限公司</td>\n      <td>本发明提供一种显示面板，显示面板的图块具有随机排布的特性，能够消除由于周期性排布带来的鬼影问...</td>\n      <td>9</td>\n      <td>一种显示面板及生成随机图块坐标的方法&lt;sep&gt;本发明提供一种显示面板，显示面板的图块具有随机...</td>\n    </tr>\n    <tr>\n      <th>20837</th>\n      <td>4c89a2b2bd405456e316a35411297b0f</td>\n      <td>一种确定机器类通信下行控制信道重复次数的方法及基站</td>\n      <td>电信科学技术研究院</td>\n      <td>本发明实施例涉及无线通信技术领域，特别涉及一种确定机器类通信下行控制信道重复次数的方法及基站...</td>\n      <td>0</td>\n      <td>一种确定机器类通信下行控制信道重复次数的方法及基站&lt;sep&gt;本发明实施例涉及无线通信技术领域...</td>\n    </tr>\n    <tr>\n      <th>20838</th>\n      <td>07870847de45f9e8fd081bd2edc7c252</td>\n      <td>一类层状钙钛矿型纳米氧化物的超级电容器电极材料</td>\n      <td>云南大学</td>\n      <td>本发明涉及一类层状钙钛矿型纳米氧化物的超级电容器电极材料，它具有化学式为LXMO的214型层...</td>\n      <td>21</td>\n      <td>一类层状钙钛矿型纳米氧化物的超级电容器电极材料&lt;sep&gt;本发明涉及一类层状钙钛矿型纳米氧化物...</td>\n    </tr>\n  </tbody>\n</table>\n<p>20839 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "pattern = '\\s+|<i>|</i>|<br>'\n",
    "\n",
    "testA_df = pd.read_json('data/testA.json', encoding='utf8', lines=True)\n",
    "label = pd.read_csv(\"results/62/5/submit (6452_bactrans_patents28000high_4epochs_rdrop result_0 6453).csv\", encoding='utf8')\n",
    "testA_df['label'] = label['label']\n",
    "testA_df['title'] = testA_df['title'].apply(lambda x: x.strip())\n",
    "testA_df['abstract'] = testA_df['abstract'].fillna('').apply(lambda x: x.strip())\n",
    "testA_df['text'] = testA_df['title'].str.lower() + '<sep>' + testA_df['abstract'].str.lower()\n",
    "testA_df['text'] = testA_df['text'].replace('<i>|</i>', '', regex=True)\n",
    "testA_df[['label', 'text']].to_csv('data/test/testA_6453.tsv', index=None, sep='\\t')\n",
    "\n",
    "testA_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "testA_df = pd.read_json('data/testA.json', encoding='utf8', lines=True)\n",
    "submit_df = pd.read_csv('submit/submit (voting fusion 0.5940).csv', encoding='utf8')\n",
    "abstracts = np.asarray(testA_df['abstract']).transpose().tolist()\n",
    "count = 0\n",
    "for i in range(len(abstracts)):\n",
    "    if str(abstracts[i]).__contains__('光伏'):\n",
    "        submit_df['label'].iloc[i] = 16\n",
    "        count += 1\n",
    "submit_df.to_csv('submit/submit (光伏).csv', encoding='utf8', index=False)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "testA_df = pd.read_json('./data/testA.json', encoding='utf8', lines=True)\n",
    "testA_df['label'] = pd.read_csv('results/63/6/test_results_0.txt', encoding='utf8', header=None, names=['label'])['label']\n",
    "testA_df[['id', 'label']].to_csv('results/63/6/submit (6453_bactrans_sampling13_4epochs_rdrop result_0).csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
