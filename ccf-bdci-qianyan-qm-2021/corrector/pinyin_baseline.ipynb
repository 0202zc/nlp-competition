{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pypinyin"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(100000, 100000)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(filepath_or_buffer='../test.tsv', sep='\t', header=None, names=['sentence1', 'sentence2'])\n",
    "sentence1 = np.asarray(data.iloc[:, 0:1]).tolist()\n",
    "sentence2 = np.asarray(data.iloc[:, 1:2]).tolist()\n",
    "len(sentence1), len(sentence2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n ...]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv('practise/paddle_baseline_batch128_ernie_gram.csv', header=None, names=['label'])\n",
    "labels = np.asarray(labels.iloc[:, 0:1]).transpose().tolist()[0]\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def check_synonyms(s1, s2):\n",
    "    if len(s1) != len(s2):\n",
    "        return False\n",
    "    for i in range(len(s1)):\n",
    "        if len(s1[i]) == 1 and len(s2[i]) == 1:\n",
    "            if s1[i][0] != s2[i][0]:\n",
    "                return False\n",
    "        elif len(s1[i]) > 1 and len(s2[i]) == 1:\n",
    "            if not (s2[i][0] in s1[i]):\n",
    "                return False\n",
    "        elif len(s2[i]) > 1 and len(s1[i]) == 1:\n",
    "            if not (s1[i][0] in s2[i]):\n",
    "                return False\n",
    "        else:\n",
    "            flag = False\n",
    "            for j in range(len(s1[i])):\n",
    "                if s1[i][j] in s2[i]:\n",
    "                    flag = True\n",
    "                    break\n",
    "            if not flag:\n",
    "                return False\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集同音率：4.422000000000001%；同音数：4422\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "output = []\n",
    "\n",
    "for i in range(100000):\n",
    "    stc1 = str(sentence1[i][0])\n",
    "    stc2 = str(sentence2[i][0])\n",
    "\n",
    "    if str(sentence1[i][0]).__contains__('×') or str(sentence2[i][0]).__contains__('×'):\n",
    "        labels[i] = 0\n",
    "        output.append(str(stc1) + '\\t' + str(stc2) + '\\t' + str(labels[i]))\n",
    "    else:\n",
    "        s1 = [pypinyin.pinyin(word, heteronym=True, style=pypinyin.STYLE_NORMAL)[0] for word in sentence1[i][0]]\n",
    "        # s2 = [pypinyin.pinyin(word, heteronym=True, style=pypinyin.STYLE_NORMAL)[0] for word in sentence2[i][0]]\n",
    "\n",
    "        if len(stc1) == len(stc2):\n",
    "            for j in range(len(stc1)):\n",
    "                temp = stc2[j:] + stc2[:j]\n",
    "                s2 = [pypinyin.pinyin(word, heteronym=True, style=pypinyin.STYLE_NORMAL)[0] for word in temp]\n",
    "                if check_synonyms(s1, s2):\n",
    "                    # print(str(stc1) + '\\t' + str(stc2) + '\\t' + str(results[i][0]))\n",
    "                    count += 1\n",
    "                    output.append(str(stc1) + '\\t' + str(stc2) + '\\t' + str(labels[i]))\n",
    "                    labels[i] = 1\n",
    "                    break\n",
    "\n",
    "    # if check_synonyms(s1, s2):\n",
    "    #     # print(str(i + 1) + \"\\t\" + sentence1[i][0] + \"\\t\" + sentence2[i][0])\n",
    "    #     results[i][0] = 1\n",
    "    #     count += 1\n",
    "\n",
    "print('测试集同音率：{}%；同音数：{}'.format(str(count / 100000 * 100), count))\n",
    "\n",
    "for i in range(100000):\n",
    "    if str(sentence1[i][0]).__contains__('×') or str(sentence2[i][0]).__contains__('×'):\n",
    "        labels[i] = 0\n",
    "\n",
    "with open('result/test_b/synonyms_list.txt', 'w', encoding='utf8', newline='\\n') as f:\n",
    "    for row in output:\n",
    "        f.write(row + '\\n')\n",
    "\n",
    "with open(file='result/pinyin_misspelling_result_B.csv', mode='w', newline='\\n', encoding='utf8') as f:\n",
    "    for result in labels:\n",
    "        f.write(str(result) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2612,  0.4183, -0.4772,  ...,  2.1578,  0.7929, -0.6297],\n",
      "         [ 1.2110,  2.4494, -0.9218,  ..., -0.0633, -0.6502,  0.4937]],\n",
      "\n",
      "        [[-2.0419, -0.1168,  1.4402,  ..., -0.2512, -0.7106,  0.3480],\n",
      "         [-1.3805,  0.3210,  0.0187,  ..., -1.2867, -0.8113,  0.2583]]])\n",
      "tensor([[[-2.6115e-01,  4.1828e-01, -4.7717e-01, -8.9844e-02,  1.5340e-01,\n",
      "           6.4570e-01,  1.1492e+00, -1.4130e+00,  4.7688e-01, -4.8074e-01,\n",
      "          -1.2255e+00, -1.2598e-01, -4.3026e-01,  8.8944e-01,  3.9290e-01,\n",
      "          -2.8022e+00, -8.1574e-01, -4.9936e-01,  9.5791e-01, -1.2399e+00,\n",
      "          -2.9465e-01, -1.0076e+00,  1.3196e+00,  1.0353e+00,  1.4676e+00,\n",
      "           1.3698e-01,  1.3393e+00, -1.7505e+00, -7.2276e-01, -9.3909e-02,\n",
      "          -9.7429e-01, -2.7154e-01,  2.1744e+00, -3.6751e-01, -2.0378e+00,\n",
      "          -4.3832e-01,  9.0376e-01,  1.2823e-01,  2.7717e+00,  3.9006e-02,\n",
      "           9.1981e-01,  6.5431e-01, -1.0346e-01,  5.2209e-01,  3.9248e-01,\n",
      "          -6.7059e-01, -8.6908e-01, -6.8613e-02, -1.0998e+00,  1.2186e+00,\n",
      "          -1.4424e+00, -1.6905e+00, -4.3402e-01,  8.9808e-02, -1.0152e+00,\n",
      "           7.4555e-02,  1.4469e-01,  7.9893e-01, -2.1260e-01, -1.8911e+00,\n",
      "           1.3729e+00,  1.1166e+00, -4.0184e-01, -2.0655e-01, -1.3197e+00,\n",
      "          -5.3773e-01,  1.2979e+00, -1.4945e+00, -7.6133e-01,  1.5027e+00,\n",
      "          -3.9394e-01,  8.8541e-01, -4.2499e-01,  7.8506e-01,  9.6509e-01,\n",
      "          -6.9392e-01, -2.4134e-01, -5.5906e-01,  7.7073e-01, -4.9852e-01,\n",
      "           1.0175e+00,  4.7003e-01, -7.9564e-01,  4.8928e-01, -2.0729e-01,\n",
      "          -5.1427e-01, -2.9722e-01,  1.1529e+00,  1.1491e+00, -1.7375e+00,\n",
      "          -7.6357e-01, -3.7629e-01, -2.6427e-02,  6.0513e-01, -8.1977e-01,\n",
      "           1.0141e+00],\n",
      "         [ 1.2110e+00,  2.4494e+00, -9.2184e-01,  6.4992e-01, -1.9386e+00,\n",
      "           2.9640e-01,  6.1373e-02,  2.7972e-02, -2.6315e+00, -9.2159e-01,\n",
      "          -3.4157e-01,  5.0129e-01, -1.5257e-01, -1.0695e+00,  4.3841e-01,\n",
      "           3.7559e-01,  7.4101e-01,  2.4234e-01,  3.2065e-01,  8.0933e-01,\n",
      "           1.3412e+00, -1.1991e+00, -2.6793e-01,  3.8192e-01, -1.6268e+00,\n",
      "          -1.7864e+00,  2.1256e-02, -1.0215e+00,  1.1011e+00,  4.5248e-01,\n",
      "          -8.3753e-01,  9.6559e-01,  9.8927e-01,  7.6400e-01,  2.1891e-02,\n",
      "           1.0359e+00,  1.8723e+00, -9.2502e-02,  7.4115e-01, -1.9412e-01,\n",
      "          -2.6943e-01,  1.4487e+00, -4.1574e-01,  1.4233e+00, -7.3204e-01,\n",
      "          -1.1301e+00, -1.6452e+00,  3.4109e-01, -1.0266e+00,  9.8805e-01,\n",
      "           1.0200e+00, -3.2472e-02,  1.2987e+00,  3.6511e-01, -1.3376e+00,\n",
      "          -4.1696e-01, -2.1593e-01, -1.7759e-01, -4.7691e-01,  2.1164e-01,\n",
      "           6.9699e-01,  2.4447e-02, -1.5894e+00,  3.5399e-01, -9.9890e-01,\n",
      "          -5.5910e-01,  9.4171e-02,  1.6575e+00,  1.2985e+00, -3.1007e-01,\n",
      "          -8.4540e-03, -1.4404e-01,  9.2579e-01, -1.0763e+00,  8.7891e-01,\n",
      "           1.0639e+00, -6.7408e-01, -1.1310e+00,  3.9772e-01,  7.3290e-01,\n",
      "           6.3777e-01, -7.6482e-01, -7.0345e-01, -4.5924e-01,  6.3958e-01,\n",
      "          -3.9296e-01,  5.5570e-01, -1.1921e-01, -1.1094e+00, -9.4686e-01,\n",
      "          -1.0390e+00,  6.0971e-01,  4.6179e-01, -3.2948e-01,  1.0547e+00,\n",
      "          -5.7953e-01]],\n",
      "\n",
      "        [[-2.0419e+00, -1.1685e-01,  1.4402e+00,  1.8896e+00, -1.9066e-01,\n",
      "          -7.0744e-02,  4.4975e-02, -5.7062e-01,  4.7414e-01,  6.8264e-01,\n",
      "           2.1961e+00, -6.6240e-02,  1.4414e+00,  2.6004e-01,  2.1913e-01,\n",
      "          -1.0807e-01, -1.7678e+00, -1.6158e-01, -1.8125e-01, -1.2976e+00,\n",
      "           1.1509e+00, -4.4426e-01,  5.2041e-01, -1.6291e+00, -3.0110e+00,\n",
      "          -3.8034e-01, -1.0812e+00, -7.5507e-01, -4.0979e-01, -4.3232e-01,\n",
      "          -2.4020e+00, -5.5605e-01, -9.2098e-01, -5.9581e-01, -8.2539e-01,\n",
      "           3.7767e-01, -5.2941e-01,  1.0407e+00, -2.6952e+00,  4.5273e-01,\n",
      "           3.0172e-01, -8.0834e-01, -7.5175e-01,  1.3083e+00, -4.2168e-01,\n",
      "           9.9836e-01, -7.7178e-01,  1.1264e+00,  1.2809e-01, -8.6426e-02,\n",
      "           5.8867e-01,  1.1351e-01,  3.1937e-01, -1.2002e+00,  7.3546e-01,\n",
      "          -1.1587e+00, -1.2849e+00,  6.3373e-01, -1.0154e+00, -1.7724e-01,\n",
      "          -1.1073e+00, -6.0669e-01,  5.4845e-01,  1.3204e-01, -3.5237e-01,\n",
      "           2.0290e+00,  6.3241e-01,  1.2248e-01, -1.2604e-01, -2.1347e-01,\n",
      "          -8.3045e-01,  6.3301e-01, -2.6539e-02, -1.0836e+00, -7.5630e-01,\n",
      "          -1.4831e+00, -1.6650e+00, -3.3719e-01, -1.4014e+00,  1.1601e+00,\n",
      "          -2.3989e-01, -4.5496e-01,  1.6643e-01,  1.0558e+00,  5.6414e-01,\n",
      "          -5.1783e-01,  2.9481e-01, -1.6386e+00,  1.8402e+00, -1.4886e-01,\n",
      "          -1.9020e+00, -1.6702e+00,  9.9208e-01, -1.5617e-01,  1.0959e-01,\n",
      "          -2.9508e-01],\n",
      "         [-1.3805e+00,  3.2104e-01,  1.8744e-02,  3.2045e-01,  1.4990e+00,\n",
      "           6.5965e-01,  2.8883e-01,  4.6900e-01, -2.1563e-01,  2.0186e+00,\n",
      "           1.5571e+00, -1.3872e+00, -3.2451e-01,  9.8560e-01,  2.6194e-01,\n",
      "           1.4672e-01, -1.4838e-01,  1.7369e+00,  1.4312e+00, -2.4437e-01,\n",
      "           6.6495e-01,  3.9978e-01,  1.4775e-01,  1.9914e+00, -1.1859e+00,\n",
      "          -8.6606e-01,  5.8392e-01,  2.0161e-01, -6.4627e-01,  3.6599e-02,\n",
      "          -4.8914e-01, -4.2253e-01,  1.5229e+00,  1.3475e+00, -1.2140e+00,\n",
      "           1.3533e+00, -9.3324e-01,  4.5087e-01,  1.0016e+00, -3.5430e-01,\n",
      "           1.7909e-01, -3.2378e-01,  1.1093e-03,  1.2295e-01, -1.0224e+00,\n",
      "          -1.0360e+00,  1.2566e+00, -1.2022e+00, -2.7040e-01, -1.7666e+00,\n",
      "           4.1524e-01, -1.2333e+00, -1.2902e-01, -1.0788e+00,  1.0406e+00,\n",
      "           3.4366e-01, -2.3285e-01,  9.0347e-01, -3.3915e-01, -2.5065e+00,\n",
      "          -3.1446e+00,  6.0937e-01, -7.7884e-01,  6.7904e-01, -4.4078e-01,\n",
      "           1.6394e+00,  5.3975e-01, -6.5512e-01, -4.5407e-01,  4.8285e-01,\n",
      "           6.9687e-01,  4.9222e-01,  1.0936e+00, -5.2016e-01, -6.9225e-01,\n",
      "           1.0726e+00, -1.2770e+00, -7.2068e-01,  2.6408e-01, -3.7659e-01,\n",
      "           1.4495e+00, -1.0233e+00,  1.6475e+00,  8.4484e-02, -8.2982e-01,\n",
      "           3.3786e-01,  8.3415e-01, -3.6278e-03, -5.1143e-01, -1.6079e+00,\n",
      "          -4.0427e-01, -7.4053e-01,  2.0995e-01, -9.6377e-01,  1.7234e+00,\n",
      "           2.3000e-01]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.randn(2, 2, 768)\n",
    "index = torch.LongTensor([j for j in range(768 // 8)])\n",
    "for i in range(len(t.shape) - len(index.shape)):\n",
    "    index = torch.unsqueeze(index, 0)\n",
    "index = index.repeat(t.shape[0], t.shape[1], 1)\n",
    "print(t)\n",
    "print(torch.gather(t, -1, index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}